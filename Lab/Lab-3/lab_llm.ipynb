{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497b74ed0b990037",
   "metadata": {},
   "source": [
    "# Lab on Prompting and fine-tuning\n",
    "\n",
    "In this lab, we will cover the basics that are needed to perform the homework 2 on Prompting and Fine-Tuning LLMs. As you will see in the next section, you will see that the order of the lab is sligthly different from the HW. This is intentional, as the fine-tuning of a model will take additional time, whereas the Few-Shot exercise (2) builds on the knowledge of exercise 1.\n",
    "\n",
    "## Layout of the Lab\n",
    "\n",
    "We will cover the following in the first part;\n",
    "\n",
    "1. Preparation of the `model` and `dataset`,\n",
    "2. Pre-processing of the `dataset`, using `jinja2` templating,\n",
    "3. Running inference experiments.\n",
    "\n",
    "In the second part, we will focus on;\n",
    "\n",
    "1. Fine-Tuning a model,\n",
    "2. Hyper-parameters to consider,\n",
    "3. Running inference with a fine-tuned model.\n",
    "\n",
    "In the third part, which will likely be mostly the 2nd lab (next week), focusses on ;\n",
    "1. Few-shot Learning,\n",
    "2. Creating context,\n",
    "3. Running inference experiemnts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d16bf6f9c4752f",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "Just to be sure, run the following to install (any missing) dependencies. You only have to run this once if you have persistent storage for you `venv` or `conda` environment. However, jsut to be shure, you might want to run this again. \n",
    "\n",
    "> Depending on your bandwith, disk, and CPU this might take a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffb0dfeb61e459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:46:58.201465Z",
     "start_time": "2024-10-19T12:46:57.474694Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the packages, if you run this a second time with persistent storage, feel free to skip.\n",
    "%pip install numpy~=1.26.0 torch~=2.2.1 transformers accelerate datasets bitsandbytes sentencepiece peft accelerate nbconvert==6.5.4 pypdf2==2 \"lxml[html_clean]\" notebook-as-pdf seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba03fea5ba1506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:00.006811Z",
     "start_time": "2024-10-19T12:46:58.207759Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This will be (most) of the packages that y0ou will need during the lab.\n",
    "# Make sure to run this cell each time you (re-)start the IPython kernel.\n",
    "\n",
    "import textwrap\n",
    "import warnings\n",
    "from importlib import metadata\n",
    "\n",
    "import datasets\n",
    "import jinja2\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965ab32696e4800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:00.083643Z",
     "start_time": "2024-10-19T12:47:00.077933Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def display_dataset_description(name: str, dataset: datasets.DatasetDict):\n",
    "    \"\"\"Helper method to display information about splits in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        name (): \n",
    "        dataset (): \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\"\"\"\n",
    "    Args:\n",
    "        name (): \n",
    "        dataset (): \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    split_info = []\n",
    "    for k, ds in dataset.items():\n",
    "        split_info.append(f\"<tr><td><strong>{k.capitalize()} Samples:</strong></td><td>{len(ds)}</td></tr>\")\n",
    "    html_content = f\"\"\"\n",
    "    <h2>Dataset info</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>Dataset Name:</strong></td><td>{name}</td></tr>\n",
    "        {\"<br>\".join(split_info)}\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "def get_available_device():\n",
    "    \"\"\"Helper method to find best possible hardware to run\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), \"cuda\"\n",
    "    \n",
    "    # Check if ROCm is available\n",
    "    if torch.version.hip is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"rocm\"), \"rocm\"\n",
    "    \n",
    "    # Check if MPS (Apple Silicon) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu'), \"mps\"\n",
    "    \n",
    "    \n",
    "    # Fall back to CPU\n",
    "    return torch.device(\"cpu\"), \"cpu\"\n",
    "\n",
    "def get_installed_version(package_name):\n",
    "    with warnings.catch_warnings():\n",
    "        # Supress warnings from packages that have missing attributes that metadata will complain about.\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        distribution = metadata.Distribution()\n",
    "        try:\n",
    "            return distribution.from_name(package_name).version\n",
    "        except metadata.PackageNotFoundError:\n",
    "            return \"Not installed\"\n",
    "\n",
    "\n",
    "def display_configuration():\n",
    "    # Check device info\n",
    "    device, backend = get_available_device()\n",
    "\n",
    "    # Torch version\n",
    "    torch_version = torch.__version__\n",
    "\n",
    "    # HuggingFace Transformers version\n",
    "    transformers_ver = transformers.__version__\n",
    "\n",
    "    # BitsAndBytes version (if available)\n",
    "    bitsandbytes_version = get_installed_version(\"bitsandbytes\")\n",
    "\n",
    "    # Check for GPU-specific details if CUDA or ROCm is available\n",
    "    if device.type == \"cuda\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.cuda\n",
    "    elif device.type == \"rocm\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.hip\n",
    "    else:\n",
    "        cuda_device_count = 0\n",
    "        cuda_device_name = \"N/A\"\n",
    "        cuda_version = \"N/A\"\n",
    "\n",
    "    # Prepare HTML formatted output for better display in a notebook\n",
    "    html_content = f\"\"\"\n",
    "    <h2>System Configuration</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>PyTorch version:</strong></td><td>{torch_version}</td></tr>\n",
    "        <tr><td><strong>Device:</strong></td><td>{device} (Backend: {backend})</td></tr>\n",
    "        <tr><td><strong>CUDA/ROCm version:</strong></td><td>{cuda_version}</td></tr>\n",
    "        <tr><td><strong>GPU count:</strong></td><td>{cuda_device_count}</td></tr>\n",
    "        <tr><td><strong>GPU name:</strong></td><td>{cuda_device_name}</td></tr>\n",
    "        <tr><td><strong>Hugging Face Transformers version:</strong></td><td>{transformers_ver}</td></tr>\n",
    "        <tr><td><strong>BitsAndBytes version:</strong></td><td>{bitsandbytes_version}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "deterministic_config: transformers.GenerationConfig = transformers.GenerationConfig(do_sample=False, max_length=100, min_length=75, repetition_penalty=1.19)\n",
    "\n",
    "def label_mapper(label: int) -> str:\n",
    "    \"\"\"Map label from int to string!\"\"\"\n",
    "    return ['Negative', \"Positive\"][label]\n",
    "\n",
    "def simple_truncate_text(row, max_length=50, tokenizer: transformers.PreTrainedTokenizerFast = None):\n",
    "    \"\"\"Example of a simple truncation method text, based on token count.\n",
    "    \n",
    "    You might want to perform 'smarter' truncation / summarization as a level, instead of simply cutting of after `max_length` tokens.\n",
    "    \n",
    "    Examples:\n",
    "        You might want to partially-apply the function, to provide a different tokenizer:\n",
    "        ```python3\n",
    "        from functools import partial\n",
    "        some_other_tokenizer = transformers.AutoTokenizer.from_pretrained('your_fave_tokenizer')\n",
    "        partial_simple_truncate = partial(simple_truncate_text, tokenizer=some_other_tokenizer)\n",
    "        ```\n",
    "    Args:\n",
    "        row (datasets....): Single instance or row of dataset.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int, 150): the maximum length of text to be processed. Defaults to 150.\n",
    "        tokenizer (transformers.PreTrainedTokenizer, `fast_tokenizer`): the tokenizer to use. Defaults to `fast_tokenizer`.\n",
    "    \n",
    "    Notes:\n",
    "        This function requires all cells above to be run.\n",
    "    \"\"\"\n",
    "    token_representation = tokenizer.batch_encode_plus(row['text'], max_length=max_length, truncation=True)['input_ids']\n",
    "    text_representation = tokenizer.batch_decode(token_representation, skip_special_tokens=True)\n",
    "    row['text'] = text_representation\n",
    "    return row\n",
    "\n",
    "def generate(\n",
    "        input_text: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        generation_config: transformers.GenerationConfig = deterministic_config,\n",
    ") -> str:\n",
    "    \"\"\"Helper method to generate a sample from the model, pre-conditioned on the input-text 'Prompt'.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): Input text to be conditioned on.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model provided.\n",
    "        model (transformers.PreTrainedModel): Pre-trained model to perform text generation with.\n",
    "        \n",
    "    Returns:\n",
    "        Generate text by the pre-conditioned model.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids, generation_config=generation_config)\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec69498a66f823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:00.713042Z",
     "start_time": "2024-10-19T12:47:00.677788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve 'best' backend and device\n",
    "device, backend = get_available_device()\n",
    "\n",
    "# Default bfloat16, because there is a lot of optimization\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Optional bits-and-bytes configuration for additional quantization.\n",
    "bab_conf = None\n",
    "if backend == 'cuda':\n",
    "    # If you want, you can further quantize on CUDA devices (linux and WSL)\n",
    "    # However, this is more for you to explore than anything else.\n",
    "    bab_conf =  transformers.BitsAndBytesConfig(\n",
    "        load_in_8bit=False\n",
    "    )\n",
    "\n",
    "# Call the display_configuration() function in your Jupyter notebook to show the configuration\n",
    "display_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde36c45670e1e",
   "metadata": {},
   "source": [
    "# Part 1: Preparing all the things\n",
    "\n",
    "Before we get started with our small lab experiment, we need to make sure that everything is prepared. Let's get started with setting up a small language model, and and loading and preparing the data.\n",
    "\n",
    "Recall from the lecture that this consists of the following 'recipe'.\n",
    "\n",
    "1. Load the model and data.\n",
    "   1. Load pre-trained or fine-tuned model\n",
    "   2. Load dataset and tokenize\n",
    "2. Run the data through the model\n",
    "3. Perform experiments (+ Analysis)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63a69f16551143",
   "metadata": {},
   "source": [
    "## Step 1: Preparing The Model\n",
    "Loading the model and see that it work, we will use the Flan-T5 model by Google / DeepMind. This model is tiny and should be fast enough even on lower powered hardware!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb66d1eca874f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:04.044828Z",
     "start_time": "2024-10-19T12:47:02.575426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create tokenizer for flan family\n",
    "family: str = \"google/flan-t5\"\n",
    "\n",
    "# For the Lab we will use a small model, just to provide some insight into usability.\n",
    "model: str = f\"-small\"\n",
    "\n",
    "model_name: str = f\"{family}{model}\"\n",
    "# Create tokenizer\n",
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "dtype = torch.bfloat16\n",
    "# Instantate model and load to the correct device.\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map='cpu',\n",
    "    torch_dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75133491d38afc8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:07.410150Z",
     "start_time": "2024-10-19T12:47:04.058741Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Here we check that everything is working, note that the story should be quite bad, as T5 is not really trained to tell us stories.\n",
    "\n",
    "input_text = \"Write a story about a dog and a boy playing with a ball on a boat with sailors.\"\n",
    "\n",
    "display(\n",
    "    Markdown(\"### Generated text by the LLM\"),\n",
    "    Markdown(f\"> {input_text}\"),\n",
    "    Markdown(\n",
    "        generate(\n",
    "            input_text=input_text,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "    ),\n",
    "    Markdown(\n",
    "        generate(\n",
    "            input_text=input_text,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            generation_config=transformers.GenerationConfig(do_sample=True, max_length=100, min_length=75, repetition_penalty=1.19)\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d787247f9559eb",
   "metadata": {},
   "source": [
    "## Step 1.5 Preparing the Data.\n",
    "\n",
    "As we will be working with a Semtiment 'classification' task, as the only labels are `Postive` (`1`) or `Negative` (`0`). First, we will need to load the appropriate dataset (`standfordnlp/imbd`), which contains movie reviews and their respective label. During the rest of the lab, we will further investigate how to do pre-processing of the data, run (different types of) inference, and perform fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727f6b877435832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset name\n",
    "data_name: str = 'stanfordnlp/imdb'\n",
    "\n",
    "# Load dataset, and assign splits to variables\n",
    "dataset: datasets.DatasetDict = datasets.load_dataset(data_name)\n",
    "train_set: datasets.Dataset = dataset['train']\n",
    "test_set: datasets.Dataset = dataset['test']\n",
    "# This unsupervised split is not used in the rest of the notebook.\n",
    "unsup: datasets.Dataset = dataset['unsupervised']\n",
    "\n",
    "# Give an overview\n",
    "display_dataset_description(data_name, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a3d7cad2c7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = train_set[1231]\n",
    "review_1, label_1 = sample['text'], label_mapper(sample['label'])\n",
    "sample = train_set[15442]\n",
    "review_2, label_2 = sample['text'], label_mapper(sample['label'])\n",
    "\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(f\"\"\"\\\n",
    "        | *Example*                 | Label     |\n",
    "        |:--------------------------|:---------:|\n",
    "        | {review_1}                | {label_1} |\n",
    "        | {review_2}                | {label_2} |\n",
    "        \"\"\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e889eb982bd5568",
   "metadata": {},
   "source": [
    "Next we will create some dataloader to ensure that we can quickly load data into the model, making the rest of the cells load a little faster.\n",
    "\n",
    "Let's also define some library functions, that we can use to calculate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52aba2cc1c8819d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:13.851992Z",
     "start_time": "2024-10-19T12:47:13.849742Z"
    }
   },
   "outputs": [],
   "source": [
    "# First determine some hyper-parameters, this should be fine on even a small model and CPU only\n",
    "\n",
    "# If you have a GPU / powerful machine, feel free to increase the following\n",
    "batch_size = 1\n",
    "test_samples = 100\n",
    "max_iterations = test_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3b324d7a9e3b2",
   "metadata": {},
   "source": [
    "# Approach 1: Simple Prompting\n",
    "\n",
    "Rather than going straight into a complex solution, let's first see what we can achieve by letting the model predict the output.\n",
    "\n",
    "\n",
    "> Note, I annotate the 'steps' in comments. There might be code sections that we will fill in during the lab, annotated with.\n",
    "\n",
    "\n",
    "```python\n",
    "# YOUR CODE GOES HERE!\n",
    "# END OF YOUR CODE!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaae94fd14c7426",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def simple_prompt_function(batch, tokenizer=tokenizer, max_length=150):\n",
    "    \"\"\"Simple prompt preparation function.\"\"\"\n",
    "    stringified_representation = list(map(lambda x: f\"Positive/Negative? {x})\", batch['text']))\n",
    "    batch['text'] = stringified_representation\n",
    "    return batch\n",
    "\n",
    "def simple_template_function(batch, template=None, tokenizer=tokenizer, max_length=150):\n",
    "    \"\"\"Mapping function using a template. Note, we will show in the lab to set this up.\"\"\"\n",
    "    stringified_representation = [template.render(review=review) for review in batch['text']]\n",
    "    batch['text'] = stringified_representation\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ff65650b3f67f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:18.648338Z",
     "start_time": "2024-10-19T12:47:18.509165Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sub_sampled_set = test_set.shuffle(seed=123).take(test_samples)\n",
    "tokenized_eval_dataset = (\n",
    "    sub_sampled_set\n",
    "    .map(simple_prompt_function, batched=True)\n",
    "    .map(lambda batch: tokenizer(batch['text']))\n",
    ")\n",
    "\n",
    "# TODO: Let's re-write to use a template!\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Display the de-tokenized text\n",
    "display(\n",
    "    Markdown('### What the model `sees`'),\n",
    "    Markdown(\n",
    "        f\"\"\"{tokenized_eval_dataset[0]['input_ids'][:100]}...\"\"\"\n",
    "    ),\n",
    "    Markdown('### What we would `see`'),\n",
    "    Markdown(\n",
    "        f\"\"\"{tokenizer.decode(tokenized_eval_dataset[0]['input_ids'], skip_special_tokens=True)}\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d47c35bbf7fd5",
   "metadata": {},
   "source": [
    "# Part 2: Running Inference\n",
    "\n",
    "Now that we got the setup out of way, we can start 'running experiments'. In short this boils down to performing 3 steps;\n",
    "\n",
    "1. Choosing your hyper-parameters and choosing appropriate levels\n",
    "2. Getting a script ready to run your experiments\n",
    "3. ** Run the experiments.** (Or, an excellent time to get coffee :P)\n",
    "\n",
    "This part of the lab will focus on that last point, to ensure that you can run your experiment efficiently, in the tutorial we are going to fix some issues with the code below, and make it run *fast*er (with some caveats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8a18142c5a73",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_fn(data_loader, model=model):\n",
    "    labels_list = []\n",
    "    prediction_list = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(model.device), batch['attention_mask'].to(model.device), batch['label'].to(model.device)\n",
    "        outputs = model.generate(\n",
    "          input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          max_length=5,\n",
    "          do_sample=False,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        labels_list.append(label.tolist())\n",
    "    return prediction_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84225092638034ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:53:50.859356Z",
     "start_time": "2024-10-19T12:53:47.137096Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE \n",
    "# 5. Set the format of the dataset to PyTorch Tensors\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_eval_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "# And run the evaluation\n",
    "predictions_list, labels_list = evaluate_fn(eval_loader, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd2c5784bac4f6",
   "metadata": {},
   "source": [
    "### Retrieving the Results\n",
    "\n",
    "Lastly, we will inspect the results of this 'experiment'. Think about some of the caveats, and how to addres them in code (don't worry, the HW does not have (all) caveats), but it is good to be aware of them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db466e0dc84ff41e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:54:42.385866Z",
     "start_time": "2024-10-19T12:54:42.367955Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_evaluation(predictions_list, labels_list):\n",
    "    flat_predictions = list(itertools.chain(*predictions_list))\n",
    "    flat_labels = list(itertools.chain(*labels_list))\n",
    "    \n",
    "    label_lut = defaultdict(lambda *args: -1, {'positive': 1, 'negative': 0})\n",
    "    predictions = list(map( lambda x: label_lut[x.split(' ')[0].lower()], flat_predictions))\n",
    "    \n",
    "    \n",
    "    accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    unknown = sum(map(lambda x: x[0] == -1, zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    \n",
    "    return accuracy, unknown\n",
    "\n",
    "\n",
    "accuracy, unknown = get_evaluation(predictions_list, labels_list)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4854c7cca9ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:47:37.171833Z",
     "start_time": "2024-10-19T12:47:37.161781Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_peft_details(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return  trainable_model_params, all_model_params, (100 * trainable_model_params) / all_model_params\n",
    "\n",
    "def tokenize_function(\n",
    "        batch,\n",
    "        prefix='Is the following Positive or Negative?\\n',\n",
    "        post_fix='\\nAnswer: '):\n",
    "\n",
    "    updated_text = [f\"{prefix}{review}{post_fix}\" for review in batch[\"text\"]]\n",
    "    batch['text'] = updated_text\n",
    "    # We also set the 'response', i.e., what the model should learn\n",
    "    batch['labels'] = tokenizer(['Positive' if label == 1 else 'Negative' for label in batch[\"label\"]], truncation=True, padding='max_length', return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def train_model(\n",
    "        peft_model,\n",
    "        peft_training_args,\n",
    "        train_set,\n",
    "        test_set = None,\n",
    "        output_dir: str = 'llm-lab',\n",
    ") -> Tuple[transformers.Trainer, peft.PeftModel]:\n",
    "    peft_trainer = transformers.Trainer(\n",
    "        model=peft_model,\n",
    "        args=peft_training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=test_set,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "    # Pre-train the model\n",
    "    peft_trainer.train()\n",
    "    # Set the fine-tuned model to evaluate, to remove non-deterministic\n",
    "    #  behavior.\n",
    "    peft_model.eval()\n",
    "    return peft_model, peft_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beae1ca02849daa",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data and Model\n",
    "\n",
    "Let's continue by preparing a model, note that a lot of this is 'boiler-plate', to reduce the trainign time considerably.\n",
    "\n",
    "If you are interested, and/or want to apply this to your project, we recommend looking into (HF tutorials of) the following:\n",
    "\n",
    "1. Quantization, where and how to apply it. For fine-tuning, we use this in the `LoraConfig`, or Low-Rank Adaptation config, which approximates the full-rank of the gradient with a lower-rank decomposition, thereby considerably reducign the overehad brought by the back-propagation\n",
    "2. Data-types, and when to use them; besides working well for training, inference may also benefit from quantization. In general, experiments are run in 'half' precision (`torch.float16` or `torch.bfloat16`), but lower preicsion exists as well (as low as 1 bit (XOR-quantization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36f44454034e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of hyper-parameters.\n",
    "RANK = 16               # Rank used in model update (lower is faster, less precise)\n",
    "ALPHA = 32              # Scaling factor for update (âˆ†W x dy ALPHA/RANK)           \n",
    "DROPOUT = 0.05          # Regularization term\n",
    "TRAIN_BATCH_SIZE = 32   # Number of samples\n",
    "TRAIN_EPOCHS = 5        # Total number of training steps.\n",
    "\n",
    "# If you want to save some time, you can store checkpoints, and load them, to create multiple levels\n",
    "# in a single run. Do note, that huggingface by default uses learning-rate scheduling, so this may\n",
    "# affect your results a bit.\n",
    "\n",
    "# The modules are specific to the model itself.\n",
    "MODULES = None #   ['o', 'k', 'q', 'v', '*'] # 'or any other identifier of weights.\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "# TODO: Decide the levels for your experiment. These can be any of the \n",
    "# aforementioned parameters, or any other hyper-parameter.\n",
    "config = lora_config = LoraConfig(\n",
    "    r=RANK,\n",
    "    lora_alpha=ALPHA,\n",
    "    target_modules=MODULES,\n",
    "    lora_dropout=DROPOUT,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(\n",
    "    model=model,\n",
    "    peft_config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a347b69f67397",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "pft, orig, pct = get_peft_details(peft_model)\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Parameter        | Statistic |\n",
    "            |:-----------------|:----------|\n",
    "            | Original         | {orig}    |\n",
    "            | PEFT             | {pft}     |\n",
    "            | Percentage       | {pct:.2f}%|\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e5beba93ce2e0",
   "metadata": {},
   "source": [
    "## Step 2: And Lift-off (ish)\n",
    "Let's do a round of training, and look at'er go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179dc6b36b4e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the number of tokens is a level, you might need to change this\n",
    "train_dataset = (\n",
    "    train_set\n",
    "    .map(limit_to_100_tokens, batched=True)\n",
    "    .map(\n",
    "        tokenize_function, batched=True\n",
    "    )\n",
    "    .map(\n",
    "        lambda batch: tokenizer.batch_encode_plus(\n",
    "            batch['text'],\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "        ), batched=True\n",
    "    )\n",
    ")\n",
    "# Ensure we can effectively use the model\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "output_dir = 'llm_lab/t5-small'\n",
    "peft_model, peft_trainer = train_model(\n",
    "    peft_model=peft_model,\n",
    "    peft_training_args=peft_training_args,\n",
    "    output_dir=output_dir,\n",
    "    train_set=train_dataset,\n",
    "    test_set=None,\n",
    ")\n",
    "peft_model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db091d66fe793423",
   "metadata": {},
   "source": [
    "### Step 2.1 Let's evaluate the model...\n",
    "\n",
    "Can you think fo some caveats fo the model, what would happen if:\n",
    "\n",
    "1. We change the prompt after training?\n",
    "2. We change the input length of the model?\n",
    "3. We want to include additional sentiments, such as `neutral`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9012343dbc0859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    test_set\n",
    "    .map(limit_to_100_tokens, batched=True)\n",
    "    .map(\n",
    "        tokenize_function, batched=True\n",
    "    )\n",
    "    .map(\n",
    "        lambda batch: tokenizer.batch_encode_plus(\n",
    "            batch['text'],\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "        ), batched=True\n",
    "    )\n",
    ")\n",
    "# Ensure we can effectively use the model\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=15,  # Feel free to lower / higher this\n",
    "    shuffle=False,  # Shuffling not needed during evaluation\n",
    "    num_workers=2,  # Feel free to set this to -1 or 1, esp. on CPU \n",
    "    prefetch_factor=10,\n",
    ")\n",
    "result, unknown = evaluate_fn(test_dataloader, model=peft_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce5d5df9ad4174",
   "metadata": {},
   "source": [
    "# Part 3: Few-Shot inference\n",
    "\n",
    "(Before) next time, the notebook will be updated to contain the relevant few-shot parts. Stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f95e9c93909a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement me!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
