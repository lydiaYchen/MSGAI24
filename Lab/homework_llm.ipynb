{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828e8adc02ee8908",
   "metadata": {
    "tags": [
     "hide-student-submission"
    ]
   },
   "source": [
    "# Homework Prompt Design\n",
    "\n",
    "In the last lab we saw that there are different ways to get to the goal of the homework is to apply DoE to guage the effectiveness of different approaches. I.e.,  evaluate different algorithms/approaches that perform classification, and leveraging. **Design of Experiments**. \n",
    "\n",
    "\n",
    "This homework consists the following 4 main parts, 1 facultative exercise to get to know a useful templating language, and 1 bonus exercise. Note the course staff reserves the right to provide corrections to this notebook and/or corresponding code.\n",
    "\n",
    "**N.B.**, this homework is both about using different techniques, and applying DoE. Its purpose is *not* to obtain a State-of-the-Art result, but rather to get to know different methods, understand their respective merrits, and applying them properly.\n",
    "\n",
    "## Submitting the Homework to Ilias\n",
    "**N.B.** To submit this homework, you must render this notebook as a PDF, run the following command in the commandline. Make sure to test this command;\n",
    "\n",
    "```bash\n",
    "jupyter-nbconvert --to pdfviahtml  homework-reference.ipynb --TagRemovePreprocessor.remove_input_tags='{\"hide-cell\",\"hide-student-submission\"}'  --TagRemovePreprocessor.remove_all_outputs_tags='{\"remove-output\"}'         \n",
    "```\n",
    "\n",
    "Before submitting make sure your notebook adheres to the following:\n",
    "\n",
    "1. None of the cells that are tagged as `keep-output` or `hide-cell` are deleted, these are key for the review of your code.\n",
    "2. You have verified that your submission PDF contains all your complete answers, note that;\n",
    "   * cells annotated with `hide-cell` will have their input removed,\n",
    "   * cells annotated with `remove-output` will have their output removed,\n",
    "   * cells annotated with `hide-student-submission` will have their input removed, e.g., this cell\n",
    "   \n",
    "3. Any cells you have added are either: properly annotated with `keep-output` or `hide-cell`, or are manually cleaned.\n",
    "\n",
    "> ⚠️ The course staff reserves the right to withold awarding (partial) points to any of the (sub)exercises if your submitted PDF and notebook do not adhere to these requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4b24275176c83",
   "metadata": {
    "tags": [
     "hide-student-submission"
    ]
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e028ac06b9be99",
   "metadata": {
    "tags": [
     "keep-output"
    ]
   },
   "source": [
    "# Homework 2 Submission\n",
    "\n",
    "| **Detail**      | **Description**  |\n",
    "|-----------------|------------------|\n",
    "| **Name**        | YOUR NAME HERE   |\n",
    "| **Student No.** | YOUR STUDENT NO. |\n",
    "| **Year**        | 20204            |\n",
    "| **Course**      | MSGAI            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30aa0f715bb80f2",
   "metadata": {
    "tags": [
     "hide-student-submission"
    ]
   },
   "source": [
    "# Before we get started\n",
    "\n",
    "This notebook seems long, but *most of the code* provides a starting point for the objective of this homework; *basic prompt-design and DoE*.\n",
    "Read each exercise carefully, you might find some hints here and there in the provided code!\n",
    "\n",
    "## Homework Overview\n",
    "\n",
    "This homework consists of the following three parts, each consisting of some implementation, and design of experiment. We provide skeleton code to perform the experiments, but you may wish to deviate from it. We recommend doing the exercises in the provided order.\n",
    "\n",
    "1. Zero-shot / Instruction based prediction.\n",
    "2. Few-shot / Example based prediction.\n",
    "3. Fine-Tuning / Learning based prediction.\n",
    "\n",
    "Each exercise consists of;\n",
    "1. A minor implementation of the main concept (see above, except for the `Fine-Tuning / Learning exercise`).\n",
    "2. Design-of-Experiments. We provide a (mostly filled out) example in Exercise 3 that you may wish to use in Exercises 1 and 2.\n",
    "3. Analysis of the DoE results, using ANOVA analysis, herein you need to check the model assumptions.\n",
    "\n",
    "Additionally, there is ONE bonus exercise (2.1.3), worth a maximum of $10$ points, which we recommend tackling last.\n",
    "3. (Bonus) Classification based prediction / Anything you want. Note, contact the TA before starting this BONUS. This BONUS will be of max. 10 points instead of the Semantic Few-Shot Prompting bonus in exercise 2. You can use the results / insights from this also in your project work.\n",
    "\n",
    "**N.B.** You can get a maximum of $60$ points in total, **with an additional maximum of $10$ bonus points**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2112c6c3dc4ff10",
   "metadata": {
    "tags": [
     "remove-output",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install dependencies (same as the env file, so you may wish to skip this if running locally / with persistent conda environment)\n",
    " %pip install python>=3.10,<4.0.0\n",
    " %pip install nbconvert==6.5.4        \n",
    " %pip install lxml_html_clean==0.3.1  \n",
    " %pip install notebook-as-pdf==0.5.0  \n",
    " %pip install bitsandbytes~=0.42.0\n",
    " %pip install configparser~=7.1.0\n",
    " %pip install datasets>=3.0.1,<4.0.0\n",
    " %pip install flake8-import-order~=0.18.2\n",
    " %pip install fqdn~=1.5.1\n",
    " %pip install isoduration~=20.11.0\n",
    " %pip install jinja2schema~=0.1.4\n",
    " %pip install jsonpointer~=3.0.0\n",
    " %pip install jupyter~=1.1.1,<2.0.0\n",
    " %pip install peft>=0.13.2,<1.0.0\n",
    " %pip install pretty-jupyter==2.0.7\n",
    " %pip install protobuf~=5.28.2,<6.0.0\n",
    " %pip install pyDOE3~=1.0.4\n",
    " %pip install researchpy~=0.3.6\n",
    " %pip install seaborn~=0.13.2\n",
    " %pip install sentence-transformers~=3.2.0\n",
    " %pip install sentencepiece~=0.2.0,<1.0.0\n",
    " %pip install tabulate~=0.9.0\n",
    " %pip install uri-template==1.3.0\n",
    " %pip install webcolors==24.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016d948465358a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:39.991412Z",
     "start_time": "2024-10-17T18:57:38.272656Z"
    },
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports used in most of the exercises\n",
    "import contextlib\n",
    "import io\n",
    "import json\n",
    "\n",
    "import textwrap\n",
    "import time\n",
    "import unittest\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from importlib import metadata\n",
    "from itertools import chain\n",
    "from os import PathLike\n",
    "from functools import partial\n",
    "from typing import Dict, List, Tuple, Union, Dict, Any\n",
    "from typing import Optional, Type\n",
    "from unittest import TextTestRunner, defaultTestLoader\n",
    "\n",
    "import datasets\n",
    "import jinja2\n",
    "import jinja2schema\n",
    "import peft\n",
    "import torch\n",
    "import transformers\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c0909905289ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:40.029695Z",
     "start_time": "2024-10-17T18:57:39.996363Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "def get_available_device() -> Tuple[torch.device, str]:\n",
    "    \"\"\"Helper method to find best possible hardware to run\n",
    "    Returns:\n",
    "        torch.device used to run experiments.\n",
    "        str representation of backend.\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), \"cuda\"\n",
    "\n",
    "    # Check if ROCm is available\n",
    "    if torch.version.hip is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"rocm\"), \"rocm\"\n",
    "\n",
    "    # Check if MPS (Apple Silicon) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu'), \"mps\"\n",
    "\n",
    "    # Fall back to CPU\n",
    "    return torch.device(\"cpu\"), \"cpu\"\n",
    "\n",
    "\n",
    "def display_dataset_description(name: str, dataset: datasets.DatasetDict) -> None:\n",
    "    \"\"\"Helper method to display information about splits in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Dataset name that was loaded. \n",
    "        dataset (datasets.DatasetDict): Dataset dict with different splits that were loaded \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    split_info = []\n",
    "    for k, ds in dataset.items():\n",
    "        split_info.append(f\"<tr><td><strong>{k.capitalize()} Samples:</strong></td><td>{len(ds)}</td></tr>\")\n",
    "    html_content = f\"\"\"\n",
    "    <h2>Dataset info</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>Dataset Name:</strong></td><td>{name}</td></tr>\n",
    "        {\"<br>\".join(split_info)}\n",
    "    </table>\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "def get_installed_version(package_name) -> str:\n",
    "    with warnings.catch_warnings():\n",
    "        # Supress warnings from packages that have missing attributes that metadata will complain about.\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        distribution = metadata.Distribution()\n",
    "        try:\n",
    "            return distribution.from_name(package_name).version\n",
    "        except metadata.PackageNotFoundError:\n",
    "            return \"Not installed\"\n",
    "\n",
    "\n",
    "def display_configuration() -> None:\n",
    "    # Check device info\n",
    "    device, backend = get_available_device()\n",
    "\n",
    "    # Torch version\n",
    "    torch_version = torch.__version__\n",
    "\n",
    "    # HuggingFace Transformers version\n",
    "    transformers_ver = transformers.__version__\n",
    "\n",
    "    # BitsAndBytes version (if available)\n",
    "    bitsandbytes_version = get_installed_version(\"bitsandbytes\")\n",
    "\n",
    "    # Check for GPU-specific details if CUDA or ROCm is available\n",
    "    if device.type == \"cuda\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.cuda\n",
    "    elif device.type == \"rocm\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.hip\n",
    "    else:\n",
    "        cuda_device_count = 0\n",
    "        cuda_device_name = \"N/A\"\n",
    "        cuda_version = \"N/A\"\n",
    "\n",
    "    # Prepare HTML formatted output for better display in a notebook\n",
    "    html_content = f\"\"\"\n",
    "    <h2>System Configuration</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>PyTorch version:</strong></td><td>{torch_version}</td></tr>\n",
    "        <tr><td><strong>Device:</strong></td><td>{device} (Backend: {backend})</td></tr>\n",
    "        <tr><td><strong>CUDA/ROCm version:</strong></td><td>{cuda_version}</td></tr>\n",
    "        <tr><td><strong>GPU count:</strong></td><td>{cuda_device_count}</td></tr>\n",
    "        <tr><td><strong>GPU name:</strong></td><td>{cuda_device_name}</td></tr>\n",
    "        <tr><td><strong>Hugging Face Transformers version:</strong></td><td>{transformers_ver}</td></tr>\n",
    "        <tr><td><strong>BitsAndBytes version:</strong></td><td>{bitsandbytes_version}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "# Call the display_configuration() function in your Jupyter notebook to show the configuration\n",
    "display_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87474e446657d1",
   "metadata": {
    "tags": [
     "hide-student-submission"
    ]
   },
   "source": [
    "## 0. Preparation\n",
    "\n",
    "In order to prepare, we will load the model and dataset that we will be using, namely the `standfordnlp/imbd` sentiment dataset, and the `google/flan-T5-small` model.\n",
    "\n",
    "You likely only need to run these setup cells once before running your code, but you might want to use the functions we provide here for certain DoE variables concerning:\n",
    "\n",
    "* Precision (`torch.float16`, `torch.float32`, `torch.bfloat16`)\n",
    "* Quantization (E.g., `bits_and_bytes_config != None`)\n",
    "* Device (E.g., `cpu` and `cuda`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2af99aeff350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:59:00.911995Z",
     "start_time": "2024-10-17T18:59:00.903703Z"
    },
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def get_model(\n",
    "        model_name: Union[str, PathLike],\n",
    "        model_type: Type[transformers.GenerationMixin] = T5ForConditionalGeneration,\n",
    "        torch_dtype: torch.dtype = torch.float16,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        bits_and_bytes_config: Optional[transformers.BitsAndBytesConfig] = None\n",
    ") -> Tuple[transformers.PreTrainedModel, transformers.PreTrainedTokenizer, Union[transformers.PreTrainedTokenizerFast, transformers.PreTrainedTokenizer]]:\n",
    "    \"\"\"Example method to instantiate a model and get a model with optional quantization (using bitsandbytes).\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Model name (huggingface name), or relative/absolute path to a pretrained model.\n",
    "        model_type (Type[transformers.PreTrainedModel]): Type of pretrained model, used to instantiate the model you wan to load.\n",
    "        torch_dtype (torch.dtype, torch.float16): Precision to load the model with. See also the BitsAndBytes documentation.\n",
    "        device (torch.device, 'cpu'): Device to run the model on.\n",
    "        bits_and_bytes_config (BitsAndBytesConfig, optional): Configuration for bitsandbytes model quantization / mixed-precisions (consider this one of your factors)\n",
    "            N.B. for fine-tuning, make sure the optimizer you want to use is available for the defined precision.\n",
    "        \n",
    "    Returns:\n",
    "        transformers.PreTrainedModel: Model instance with provided configuraiton.\n",
    "        transformers.PreTrainedTokenizer: Tokenizer instance with provided configuration.\n",
    "        transformers.PreTrainedTokenizerFast: Fast tokenizer if avilable, othersiwe a normal python based optimizer\n",
    "    \n",
    "    Notes:\n",
    "        For using the BitsAndBytes quantization configuration, an Nvidia GPU is required. For this you might want to make \n",
    "        use of the Google Collab L4 / K40 GPUs (free-tier).    \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    model: transformers.PreTrainedModel = model_type.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "        quantization_config=bits_and_bytes_config,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "    )\n",
    "    fast_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "        use_fast=True\n",
    "    )\n",
    "     \n",
    "    return model, tokenizer, fast_tokenizer\n",
    "\n",
    "def get_dataset(\n",
    "        data_name: str,\n",
    "        splits: List[str]\n",
    ") -> Tuple[datasets.Dataset, ...]:\n",
    "    \"\"\"Helper method to load huggingface dataset.\n",
    "    \n",
    "    Args:\n",
    "        data_name (str): Dataset name to load from huggingface. \n",
    "        splits (List[str]): List of splits to load and return. \n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the dataset splits.\n",
    "    \"\"\"\n",
    "    # Load dataset, and assign splits to variables\n",
    "    dataset: datasets.DatasetDict = datasets.load_dataset(data_name)\n",
    "    return tuple(dataset[split] for split in splits)\n",
    "\n",
    "def simple_truncate_text(row, max_length=50, tokenizer: transformers.PreTrainedTokenizerFast = None):\n",
    "    \"\"\"Example of a simple truncation method text, based on token count.\n",
    "    \n",
    "    You might want to perform 'smarter' truncation / summarization as a level, instead of simply cutting of after `max_length` tokens.\n",
    "    \n",
    "    Examples:\n",
    "        You might want to partially-apply the function, to provide a different tokenizer:\n",
    "        ```python3\n",
    "        from functools import partial\n",
    "        some_other_tokenizer = transformers.AutoTokenizer.from_pretrained('your_fave_tokenizer')\n",
    "        partial_simple_truncate = partial(simple_truncate_text, tokenizer=some_other_tokenizer)\n",
    "        ```\n",
    "    Args:\n",
    "        row (datasets....): Single instance or row of dataset.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int, 150): the maximum length of text to be processed. Defaults to 150.\n",
    "        tokenizer (transformers.PreTrainedTokenizer, `fast_tokenizer`): the tokenizer to use. Defaults to `fast_tokenizer`.\n",
    "    \n",
    "    Notes:\n",
    "        This function requires all cells above to be run.\n",
    "    \"\"\"\n",
    "    token_representation = tokenizer.batch_encode_plus(row['text'], max_length=max_length, truncation=True)['input_ids']\n",
    "    text_representation = tokenizer.batch_decode(token_representation, skip_special_tokens=True)\n",
    "    row['text'] = text_representation\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cdbad45c06320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:41.586814Z",
     "start_time": "2024-10-17T18:57:40.060042Z"
    },
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Create tokenizer for flan family\n",
    "family: str = \"google/flan-t5\"\n",
    "# For the Lab we will use a small model, just to provide some insight into usability.\n",
    "model: str = f\"small\"\n",
    "model_name: str = f\"{family}-{model}\"\n",
    "\n",
    "tokenizer: T5Tokenizer\n",
    "fast_tokenizer: T5TokenizerFast\n",
    "model: T5ForConditionalGeneration\n",
    "\n",
    "# NOTE, you might need to change this for different model Families\n",
    "#   as T5 family specifically is a encoder-decoder whereas most text gen. models are\n",
    "#   of type AutoModelForCausalLM.\n",
    "model_type: Type[transformers.GenerationMixin] = transformers.AutoModelForSeq2SeqLM\n",
    "# model_type: Type[transformers.GenerationMixin] = transformers.AutoModelForCausalLM\n",
    "device, backend = get_available_device()\n",
    "model, tokenizer, fast_tokenizer = get_model(\n",
    "    model_name=model_name,\n",
    "    model_type=model_type,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device,\n",
    ")\n",
    "# Set the model to Evaluation to prevent creating a computational graph\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc8a578e46a5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:44.796132Z",
     "start_time": "2024-10-17T18:57:41.603123Z"
    },
    "tags": [
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "data_name: str = 'stanfordnlp/imdb'\n",
    "splits = ['train', 'test', 'unsupervised']\n",
    "train_set, test_set, *_ = get_dataset(data_name, splits=splits)\n",
    "text, label = f\"{train_set[1239]['text'][:40]}...\", train_set[0]['label']\n",
    "display(\n",
    "    Markdown(\n",
    "f\"\"\"\n",
    "| Text  | Label   |\n",
    "|:-----:|:-------:|\n",
    "|{text} | {label} |\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292f54f98fcec96",
   "metadata": {},
   "source": [
    "## (Optional) Becoming a Jinja Ninja!\n",
    "\n",
    "As a starting point for data-manipulation, here are some exercises to get used with Jinja! We recommend looking into Jinja templating, with variables and for loops.\n",
    "As you will see, Jinja is a very flexible templating engine, that allows you to wrangle the IMDb dataset that we will use into the correct format for your experiments.\n",
    "\n",
    "In the following exercises you can see how you can:\n",
    "\n",
    "1. Render parameters in a Jinja Template\n",
    "2. Render lists in a Jinja Template\n",
    "3. Render `zip`ped list in a Jinja Template\n",
    "\n",
    "> N.B. use the test methods to see what is expected / the expected return statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838a551c4edde2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:44.862812Z",
     "start_time": "2024-10-17T18:57:44.857041Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "example_template = jinja2.Template(\n",
    "    textwrap.dedent(\n",
    "        \"\"\"\\\n",
    "        Hello my name is: {{ MY_NAME }}\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "print(example_template.render(MY_NAME=\"Your name :)\"))\n",
    "\n",
    "# Implement a template that uses variables `course` `professor` and `ta`\n",
    "# Would render `I follow MSGAIs 2024/2025 taught by Prof. L. Y. Chen, and can contact Ir. J. M. Galjaard for questions.`\n",
    "VAR_TEMPLATE = textwrap.dedent(\n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    ")\n",
    "variables_template = jinja2.Template(\n",
    "    VAR_TEMPLATE   \n",
    ")\n",
    "variables = jinja2schema.infer(VAR_TEMPLATE)\n",
    "assert set(variables.keys()) == {'course', 'professor', 'ta'}, 'Not all variables are used'\n",
    "\n",
    "# As example\n",
    "print(variables_template.render(course ='MSGAIS', professor='Lydia', ta='Jeroen'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1eca7a2b70333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:44.909692Z",
     "start_time": "2024-10-17T18:57:44.905181Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement a template that uses a variable `exercises` that contains a list of strings.\n",
    "#  it should render as a Markdown list\n",
    "# HINT: use a jinja for-loop\n",
    "list_expected = \"\"\"I need to implement:\n",
    " * Basic prompting\n",
    " * Few-shot Learning\n",
    " * Fine-Tuning\n",
    " * Bonus\"\"\"\n",
    "LIST_TEMPLATE = textwrap.dedent(\n",
    "    \"\"\"\\\n",
    "    I need to implement:{% for exercise in exercises %}\n",
    "    * {{ exercise }}{% endfor %}\n",
    "    \"\"\"\n",
    ")\n",
    "list_template = jinja2.Template(\n",
    "    LIST_TEMPLATE\n",
    ")\n",
    "variables = jinja2schema.infer(LIST_TEMPLATE)\n",
    "assert 'exercises' in set(variables.keys()), 'Exercise variables is not used!'\n",
    "print(list_template.render(exercises=['Basic prompting', 'Few-shot Learning', 'Fine-Tuning', 'Bonus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef041258f923cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:44.957926Z",
     "start_time": "2024-10-17T18:57:44.954115Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement a template that uses a variable `points_exercises` that contains a list of tuples.\n",
    "# HINT: use a jinja for-loop and variable unrolling\n",
    "zip_expected = \"\"\"I need to implement:\n",
    "* (20) Basic prompting\n",
    "* (20) Few-shot Learning\n",
    "* (20) Fine-Tuning\n",
    "* (10) Bonus\"\"\"\n",
    "\n",
    "ZIP_TEMPLATE = textwrap.dedent(\n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    ")\n",
    "zip_template = jinja2.Template(\n",
    "    ZIP_TEMPLATE\n",
    ")\n",
    "variables = jinja2schema.infer(ZIP_TEMPLATE)\n",
    "assert set(variables.keys()) == {'points_exercises'}, 'Exercise variables is not used!'\n",
    "print(zip_template.render(points_exercises=[(20, 'Basic prompting'), ( 20, 'Few-shot Learning'), (20, 'Fine-Tuning'), (10, 'Bonus')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83315bec1160669c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:45.016502Z",
     "start_time": "2024-10-17T18:57:45.010507Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Do not edit the following code.\n",
    "class TestJinjaNinja(unittest.TestCase):\n",
    "    exercises = ['Basic prompting', 'Few-shot Learning', 'Fine-Tuning', 'Bonus']\n",
    "    points = [20, 20, 20, 10]\n",
    "    def test_1_variable(self):\n",
    "        \n",
    "        check_against = \"I follow MSGAIs 2024/2025 taught by Prof. L. Y. Chen, and can contact Ir. J. M. Galjaard for questions.\"\n",
    "        course = 'MSGAIs 2024/2025'\n",
    "        professor = 'Prof. L. Y. Chen'\n",
    "        ta = 'Ir. J. M. Galjaard'\n",
    "        result = variables_template.render(course=course, professor=professor, ta=ta)\n",
    "        self.assertEqual(result, check_against)\n",
    "    \n",
    "    def test_2_list_template(self):\n",
    "        check_against = textwrap.dedent(\"\"\"\\\n",
    "        I need to implement:\n",
    "        * Basic prompting\n",
    "        * Few-shot Learning\n",
    "        * Fine-Tuning\n",
    "        * Bonus\"\"\")\n",
    "        result = list_template.render(exercises=self.exercises)\n",
    "        self.assertEqual(result, check_against)\n",
    "        \n",
    "    \n",
    "    def test_3_list_zipped(self):\n",
    "        check_against = textwrap.dedent(\"\"\"\\\n",
    "        I need to implement:\n",
    "        * (20) Basic prompting\n",
    "        * (20) Few-shot Learning\n",
    "        * (20) Fine-Tuning\n",
    "        * (10) Bonus\"\"\")\n",
    "        \n",
    "        result = zip_template.render(points_exercises=list(zip(self.points, self.exercises)))\n",
    "        self.assertEqual(result, check_against)\n",
    "        \n",
    "f = io.StringIO()\n",
    "with contextlib.redirect_stderr(f):\n",
    "    display(Markdown(\"### Exercise 0.1 Optional exercise result\"))\n",
    "    TextTestRunner(verbosity=-1).run(defaultTestLoader.loadTestsFromTestCase(TestJinjaNinja))\n",
    "    display(Markdown('---'))\n",
    "    print(f\"\\033[91m{f.getvalue()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1bf56e5e2bf4a",
   "metadata": {},
   "source": [
    "# Exercise 1: Prompt-based Evaluation (20 points total) \n",
    "Instead of fine-tuning a model specific to a problem, we can use the language model's capability to follow instructions to perform a specific task. In all these tasks, we will make use of the IMDB movie review sentiment dataset. Throughout this, and following exercises, we will be 'asking' the model to predict the sentiment (Positive or Negative). \n",
    "\n",
    "A naive idea, is ask the model simply: ``Has the following a Positive or Negative sentiment?''.\n",
    " \n",
    "In this exercise, you will;\n",
    "\n",
    "1. **Exercise 1.1**     (5 points) implement two 'Zero-Shot' prompts 'templates', that prompt the model to decide upon the semtiment without additional information\n",
    "2. **Exercise 1.2:**    (8 points) Perform DoE with different system- and/or hyper-parameters during generation, to evaluate how they impact the models performance (accuracy).\n",
    "3. **Exercise 1.3:**    (7 points) Analyse the result of your DoE experiments, usign ANOVA.\n",
    "\n",
    "\n",
    "The goal here is to evaluate the impact of different hyper-parameters and/or system-parameters on the classification accuracy of the model.\n",
    "\n",
    "> ❗One of the levels in your DoE, will be the input representation, i.e., a `simple_prompt` and a more contextual `detailed_prompt`. You will implement these Zero-Shot prompts. The simple prompt should be a mere short question, whereas the detailed prompt should give additional context, e.g., about the domain / task that is performed.\n",
    "\n",
    "\n",
    "> *N.B.* to guide you through the exericse, we annotate things you will need to implement. In the lab we will provide some example on how to tackle this.\n",
    "\n",
    " ```python\n",
    " # YOUR CODE GOES HERE!\n",
    "\n",
    " # END OF YOUR CODE!\n",
    " ```\n",
    "\n",
    "**⚠ FAIR Warning:**\n",
    "\n",
    "> YOU should make sure to store results to disk or other persisten storage, i.e. by writing to a file or saving a model. For example when you want to run with different models you should make sure that data is not accidentally overwritten!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37feec8b86609a95",
   "metadata": {},
   "source": [
    "## Exercise: 1.1 Prompt-Design (5 points)\n",
    "\n",
    "First, we ensure that we can represent the data to the model with our designed prompt, for this, you will need to implement the following behavior;\n",
    "\n",
    "1. A simple (yes/no)-like question for the prompt in `get_simple_prompt_template`. (2 points (left / right)).\n",
    "    * This should ask for a `positive`/`Positive` or `negative`/`Negative` as answer, i.e., asking to classify the sentiment of text.\n",
    "2. A detailed (contextual) question for the prompt in `get_detailed_prompt_template`. (3 points (left / right)).\n",
    "    * This should ask for a `positive`/`Positive` or `negative`/`Negative` as answer, i.e., asking to classify the sentiment of text.\n",
    "    * The question should provide additional context regarding the task that is performed (e.g., sentiment analysis, type of task that is peformed, etc.).\n",
    "\n",
    "\n",
    "**N.B**, we don't recommend using a library like `langchain` to do the homework, as they can become restrictive in the specifics that you want to use. You can opt to use it, but the course does not provide support on additional optional frameworks.\n",
    "\n",
    "**N.B.** we do recommend using Jinja to create templates for prompts. This allows to quickly transform input for your experiments for your execution of DoE.\n",
    "\n",
    "Additionally, make sure to use the appropriate `textwrap.dedent` option, if you use triple-quoted (multi-line) `str`ings! Otherwise, you will add (unintenional) whitespace `char`s!\n",
    "\n",
    "> If you are unsure how to do this, see the Preparation exercise above, as they provide some hints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236f5b0f187306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:45.055920Z",
     "start_time": "2024-10-17T18:57:45.050361Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def get_simple_prompt_template(\n",
    "        side: str = 'left',\n",
    ") -> jinja2.Template:\n",
    "    \"\"\"Implements a simple Template retrieval function, that takes as argument `review` and renders as a simple prompt.\n",
    "    Keyword Args:\n",
    "        side (str, 'left'): Position at which to add the question for the prompt.\n",
    "        \n",
    "    Returns:\n",
    "        jinja.Template that can render an argument `review`, consisting of a string represention of a review.\n",
    "    \"\"\"\n",
    "    # TODO: Implement a simple zero-shot style yes/no style QA Template.    \n",
    "    match side:\n",
    "        case 'left':\n",
    "            # TODO: Implement question first, then `review`\n",
    "\n",
    "            PROMPT_TEMPLATE = textwrap.dedent(\n",
    "                # YOUR CODE GOES HERE\n",
    "                ...\n",
    "                # END OF YOUR CODE\n",
    "            )\n",
    "        case 'right':\n",
    "            # TODO: Implement `review` first, then question\n",
    "            PROMPT_TEMPLATE = textwrap.dedent(\n",
    "                # YOUR CODE GOES HERE\n",
    "                ...\n",
    "                # END OF YOUR CODE\n",
    "            )\n",
    "    assert set(jinja2schema.infer(PROMPT_TEMPLATE).keys()) == {'review'}, \"Your template does not use the `review` argument.\"\n",
    "    return jinja2.Template(PROMPT_TEMPLATE)\n",
    "\n",
    "\n",
    "\n",
    "simple_template_l = get_simple_prompt_template(side='left')\n",
    "\n",
    "simple_template_r = get_simple_prompt_template(side='right')\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        simple_template_l.render(review='Review would go here...').replace('\\n', '<br>')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9711e7b2bed19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:57:51.676793Z",
     "start_time": "2024-10-17T18:57:51.670852Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "# RUN EVALUATION\n",
    "# Don't change the code below.\n",
    "\n",
    "def nl_to_br(inp, br: str='<br>'):\n",
    "    return inp.replace('\\n', br)\n",
    "\n",
    "example_review = f\"{train_set[1203]['text'][:142]}...\"\n",
    "simple_prompt_l = nl_to_br(simple_template_l.render(review='Review would go here...'))\n",
    "simple_example_l = nl_to_br(simple_template_l.render(review=example_review))\n",
    "\n",
    "simple_prompt_r = nl_to_br(simple_template_r.render(review='Review would go here...'))\n",
    "simple_example_r = nl_to_br(simple_template_r.render(review=example_review))\n",
    "\n",
    "display(\n",
    "    Markdown('### Exericse 1.1.1 Result'),\n",
    "    HTML(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "                <tr>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">My simple prompt (left)</th>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">My simple prompt (right)</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{simple_prompt_l}</td>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{simple_prompt_r}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">Example</th>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">Example</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{simple_example_l}</td>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{simple_example_r}</td>\n",
    "                </tr>\n",
    "            </table>\"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4be7333feb5f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:58:25.333078Z",
     "start_time": "2024-10-17T18:58:25.325696Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_detailed_prompt_template(\n",
    "        side='left',\n",
    ") -> jinja2.Template:\n",
    "    \"\"\"Implements a detailed contextual Template retrieval function, that takes as argument `review` and renders a detailed prompt\n",
    "    with contextual information.\n",
    "    \n",
    "    Keyword Args:\n",
    "        side (str, 'left'): Position at which to add the question for the prompt.\n",
    "        \n",
    "    Returns:\n",
    "        Template that can render an argument `review`, consisting of a string representation of a review.\n",
    "    \"\"\"\n",
    "    match side:\n",
    "        case 'left':\n",
    "            # TODO: Implement Question-first, Context second template.\n",
    "            PROMPT_TEMPLATE = textwrap.dedent(\n",
    "                # YOUR CODE GOES HERE\n",
    "                ...\n",
    "                # END OF YOUR CODE\n",
    "            )\n",
    "        case 'right':\n",
    "            # TODO: Implement Context-first, Question-second template.\n",
    "            PROMPT_TEMPLATE = textwrap.dedent(\n",
    "                # YOUR CODE GOES HERE\n",
    "                ...\n",
    "                # END OF YOUR CODE\n",
    "            )\n",
    "    assert set(jinja2schema.infer(PROMPT_TEMPLATE).keys()) == {'review'}, \"Your template does not use the `review` argument.\"\n",
    "    return jinja2.Template(PROMPT_TEMPLATE)\n",
    "\n",
    "detailed_template_l = get_detailed_prompt_template(\n",
    "    side='left',\n",
    ")\n",
    "detailed_template_r = get_detailed_prompt_template(\n",
    "    side='right',\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown('### Exericse 1.1.2 Result'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "        f\"\"\"\\\n",
    "        | **My simple prompt (left)** | **My simple prompt (right)** |\n",
    "        |-----------------------------|------------------------------|\n",
    "        | {simple_prompt_l}           | {simple_prompt_r}            |\n",
    "        | **Example**                 | **Example**                  |\n",
    "        | {simple_example_l}          | {simple_example_r}           |\n",
    "        \"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46592fc82afbeb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:58:26.141192Z",
     "start_time": "2024-10-17T18:58:26.135592Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "# RUN EVALUATION\n",
    "# Don't change the code below.\n",
    "example_review = f\"{train_set[1203]['text'][:142]}...\"\n",
    "\n",
    "detailed_prompt_l = nl_to_br(detailed_template_l.render(review='Review would go here...'))\n",
    "detailed_example_l = nl_to_br(detailed_template_l.render(review=example_review))\n",
    "\n",
    "detailed_prompt_r = nl_to_br(detailed_template_r.render(review='Review would go here...'))\n",
    "detailed_example_r = nl_to_br(detailed_template_r.render(review=example_review))\n",
    "\n",
    "display(\n",
    "    Markdown('### Exericse 1.1.2 Result'),\n",
    "    HTML(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "                <tr>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">My simple prompt (left)</th>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">My simple prompt (right)</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{detailed_prompt_l}</td>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{detailed_prompt_r}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">Example</th>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">Example</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{detailed_example_l}</td>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{detailed_example_r}</td>\n",
    "                </tr>\n",
    "            </table>\"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988b661f54c8242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:00:01.029080Z",
     "start_time": "2024-10-17T19:00:00.990553Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the truncated eval set.\n",
    "MAX_50_TOKENS = 50\n",
    "truncate_to_50_tokens = partial(simple_truncate_text,  max_length=MAX_50_TOKENS, tokenizer=fast_tokenizer)\n",
    "\n",
    "q1_eval_set = (\n",
    "    test_set\n",
    "    .map(truncate_to_50_tokens, batched=True)\n",
    ")\n",
    "\n",
    "truncated_example_text, label = nl_to_br(q1_eval_set[0]['text']), q1_eval_set[0]['label']\n",
    "display(\n",
    "    Markdown(\n",
    "    \"\"\"## Example of truncated data\n",
    "Do you see how the text is abruptly terminated after `I tried to like this, I`?\n",
    "\"\"\"),\n",
    "    Markdown(textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            > | **Truncated Review**          | **Label** |\n",
    "            |-------------------------------|-----------|\n",
    "            | {truncated_example_text}      | {label}   |\n",
    "            \"\"\"\n",
    "        )\n",
    "    ),\n",
    "    Markdown('---')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75acf1a1d06bc43",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Design of Experiments (8 points)\n",
    "\n",
    "In this and the following exercise, we are interested in quantifing the effect of different configurations on the zero-shot performance of the model, you will need to select at-least 3 system- and/or hyper-parameters, with each having atleast two or more (2+) levels. Recall that for the first hyper-parameter should use the (`simple` or `detailed`) prompt.\n",
    "\n",
    "Furthermore, we suggest using one or more from the following parameters in your DoE:\n",
    "\n",
    "  * The structure of each prompt (i.e., `left` and `right`)\n",
    "  * Model size, for example (`T5-flan-small`, `T5-flan-base`, `T5-flan-large`, etc.). (only recommended with GPU)\n",
    "  * Numerical precision (`torch.float16`, `torch.float32`, `torch.bfloat16`). Make sure your hardware / `PyTorch` version supports this!\n",
    "  * Quantization (only recommended with GPU with `BitsAndBytes` packages).\n",
    "  * Structured decoding (requires implememtantation).\n",
    "\n",
    "\n",
    "In short, you will need to perform;\n",
    "\n",
    "1. (8 points) Design of Experiments in code;\n",
    "    * Selection of criteria\n",
    "    * Type of factorial experiment\n",
    "    * Creation of experimental configuration\n",
    "    * Run your experiments.\n",
    "        * Depending on your chosen variables in DoE, you might need to make some minor adaptations to our provided code.\n",
    "\n",
    "> For your convenience, we have split first DoE part,  and the Design of Experiments (which you have to implement), and the ANOVA analysis into 2 cells. We strongly recommend writing data to disk/persistent storage and loading it in the next cell to make sure you can easily re-run evaluation upon restarting the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d4b57e32365da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:04:56.099561Z",
     "start_time": "2024-10-17T19:04:56.091263Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def run_q1_evaluation(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        generation_config: transformers.GenerationConfig,\n",
    "        *args,\n",
    "        **kwargs\n",
    ") -> Tuple[List[List[str]], List[List[int]]]:\n",
    "    \"\"\"Helper function to run evaluation (e.g. under different evaluations).\n",
    "    \n",
    "    Notes:\n",
    "        You likely don't need to make any changes, as likely most of your levels are;\n",
    "         * system-parameters,\n",
    "         * generation-parameters,\n",
    "         * different ways of pre-processing the review data.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): Dataloader containing the evaluation set. \n",
    "        model (transformers.PreTrainedModel): Pre-Trained model to be evaluated.\n",
    "        generation_config: Generation configuraiton, that may contain some of your hyper-parameters for DoE.\n",
    "        *args: Any additional positional args you want to add.\n",
    "    \n",
    "    Keyword Args:\n",
    "        **kwargs: Any additional keyword args you want to add.\n",
    "\n",
    "    Returns:\n",
    "        List of list containing the `str`ing representation of the models predicition.\n",
    "        List of list containing the `int`eger representation of the ground-truth label.\n",
    "    \"\"\"\n",
    "    prediction_list, label_list = [], []\n",
    "    for idx, batch in (pbar := tqdm(enumerate(dataloader), leave=False, total=len(dataloader))):\n",
    "        pbar.set_description(f'Batch {idx}')\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch[\n",
    "            'label'].to(device)\n",
    "        # YOUR CODE GOES HERE\n",
    "        ...\n",
    "        # END OF YOUR CODE\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=5,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        label_list.append(label.cpu().tolist())\n",
    "    return prediction_list, label_list\n",
    "\n",
    "\n",
    "def get_q1_sets(\n",
    "        dataset: datasets.Dataset\n",
    ") -> Tuple[datasets.Dataset, datasets.Dataset]:\n",
    "    \"\"\"Helper method to create the low and high level datasets with the `simple` and `detailed` prompt.\n",
    "    \n",
    "    Notes:\n",
    "        You likely don't need to edit this code, but feel free to extend this code, in case you want to\n",
    "        evaluate more different levels\n",
    "    \n",
    "    Args:\n",
    "        dataset (datasets.Dataset): Dataset to be mapped to a simple and detailed representation dataset.\n",
    "        \n",
    "    Returns:\n",
    "        Dataset with text mapped using the `simple_template`.\n",
    "        Dataset with text mapped using the `detailed_template`.\n",
    "    \"\"\"\n",
    "    # 1. Prepare the simple set (low level)\n",
    "    simple_set = (\n",
    "        dataset\n",
    "        .map(\n",
    "            lambda batch: fast_tokenizer.batch_encode_plus(\n",
    "                [simple_template.render(review=row) for row in batch['text']],\n",
    "                truncation=False,\n",
    "                padding=True,\n",
    "            ),\n",
    "            batched=True,\n",
    "        )\n",
    "    )\n",
    "    # Map to input expected by the model.\n",
    "    simple_set.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    # 2. Prepare the detailed set (high level)\n",
    "    detailed_set = (\n",
    "        dataset\n",
    "        .map(\n",
    "            lambda batch: fast_tokenizer.batch_encode_plus(\n",
    "                [detailed_template.render(review=row) for row in batch['text']],\n",
    "                truncation=False,\n",
    "                padding=True,\n",
    "            ),\n",
    "            batched=True,\n",
    "        )\n",
    "    )\n",
    "    # Map to input expected by the model\n",
    "    detailed_set.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    \n",
    "    return simple_set, detailed_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50f3347b7e6952",
   "metadata": {},
   "source": [
    "### Exericse 1.2.1 Design of Experiments\n",
    "Define your Design of Experiment configurations in the list `EXPERIMENT_CONFIGURATIONS`, you can use this list to store experiment configurations for the different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0a9dbd3bf7960",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Implement your experimental design here! Decide on hyper-parameters, levels,\n",
    "#  and type of factorial experiment you want to do.\n",
    "\n",
    "EXPERIMENT_CONFIGURATIONS: List[Dict[Any, Any]] = [\n",
    "    None\n",
    "]\n",
    "# YOUR CODE GOES HERE\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef4f0b8d3875d2",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ONLY SET THIS TO True IFF YOU NEED TO RE-RUN EXPERIMENTS, AS IT WILL\n",
    "#  OVERWRITE YOUR RESULTS.\n",
    "ALLOW_OVERWRITING_RESULTS = False\n",
    "\"\"\"\n",
    "# If you want to experiment with the side of the prompt, you will need to make\n",
    "#  some changes here.\n",
    "\"\"\"\n",
    "simple_template = get_simple_prompt_template(side='left')\n",
    "detailed_template = get_detailed_prompt_template(side='left')\n",
    "\n",
    "# Prepare datasets\n",
    "simple_set, detailed_set = get_q1_sets(q1_eval_set)\n",
    "\n",
    "\n",
    "for configuration in (exp_bar := tqdm(EXPERIMENT_CONFIGURATIONS, leave=True)):\n",
    "    # EXAMPLE CODE HERE\n",
    "    # Determine the maximum lenght given the length of your prompts. \n",
    "    # N.B. You might want to use this, but as we propose a pre-tokenized\n",
    "    simple_overhead = len(tokenizer(simple_template.render(), add_special_tokens=False)['input_ids'])\n",
    "    detailed_overhead = len(tokenizer(detailed_template.render(), add_special_tokens=False)['input_ids'])\n",
    "    \n",
    "    \"\"\"\n",
    "    # You might need to subsample the dataset to 1000, if your hardware is too slow. Make sure to report\n",
    "    #   if and how you sub-sample\n",
    "    simple_set, detailed_set = simple_set.sample(...), detailed_set.sample(...)\n",
    "    \"\"\"\n",
    "    for split, dataset in (split_bar := tqdm(zip(['simple', 'detailed'], [simple_set, detailed_set]), leave=False, total=2)):\n",
    "        q_data_loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=15,  # Feel free to lower / higher this\n",
    "            shuffle=False,  # Shuffling not needed during evaluation\n",
    "            num_workers=2,\n",
    "            prefetch_factor=10,\n",
    "        )\n",
    "        \n",
    "        begin_time = time.time()\n",
    "        prediction_list, label_list = run_q1_evaluation(\n",
    "            q_data_loader,  # This you should probably not change\n",
    "            model,  # You might need to change / load a different model for model-parameter\n",
    "            configuration,  # You might need to update some kwargs int the generation config for your exp.\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        # Create a flat version to work with.\n",
    "        prediction_list, labels_list = list(chain(*prediction_list)), list(chain(*label_list))\n",
    "        # TODO: Store your results in a way such that you can load it later!\n",
    "        # YOUR CODE GOES HERE\n",
    "        ...\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        # Map the output if we don't recognize it to \n",
    "        label_lut = defaultdict(lambda: -1, {'positive': 1, 'negative': 0})\n",
    "        \n",
    "        predictions = list(map(lambda x: label_lut[x.split(' ')[0].lower()], prediction_list))\n",
    "        \n",
    "        accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, labels_list))) / len(predictions)\n",
    "        unknown =  sum(map(lambda x: x[0] == -1, zip(predictions, labels_list))) / len(predictions)\n",
    "\n",
    "        print(f\"Accuracy ({split}): {accuracy}, Unknown: {unknown}\")\n",
    "        experiment_description = ...\n",
    "        save_path_experiment = ...\n",
    "        \n",
    "        # Write file to disk\n",
    "        save_path = Path(save_path_experiment)\n",
    "        if not save_path.parent.exists():\n",
    "            # Recursively create directory\n",
    "            save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        if save_path.is_file() and not ALLOW_OVERWRITING_RESULTS:\n",
    "            print(\"YOU ARE TRYING TO OVERWRITE AN EXISTING EXPERIMENT FILE!\")\n",
    "            raise Exception(\"Cannot overwrite existing experiment file without `ALLOW_OVERWRITING_RESULTS` flag set.\")\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            # TODO: You might want to save some additional results.\n",
    "            json.dump({\n",
    "                \"description\": experiment_description,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"unknown\": unknown,\n",
    "                \"begin_time\": begin_time,\n",
    "                \"end_time\": end_time,\n",
    "            }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d1596d80e9011",
   "metadata": {},
   "source": [
    "### Exercise 1.3 Report on DoE (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042f6d110648d9a",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Code for your evaluation of results and write a small report on the \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451491eece3c6a57",
   "metadata": {},
   "source": [
    "\n",
    "*TODO: Write your report here, using appropriate tables, and or $math$, to support your claim.*\n",
    "\n",
    "Make sure to clearly state (among others):\n",
    "\n",
    "1. Which hyper-parameters you are testing \n",
    "2. Which levels you are testing for each experiment\n",
    "3. How many repetitions you use\n",
    "4. Which design of experiment you use: full-factorial / fractional-factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129567b768b27df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a647c148276eaf5",
   "metadata": {},
   "source": [
    "# Exercise 2: Learning Through Examples 'In-Context Learning' (20 points total)\n",
    "\n",
    "Instead of asking the model its decision at face-value, in this exercise we will provide the model with a view examples. Although the jury is out on why this exactly works, the idea is that the examples allow to 'prime' the model, to understand the task better that it is going to perform.\n",
    "\n",
    "In short, this exercise consists of the following parts;\n",
    " 1. **Exercise 2.1.1:**  (2 points) Design and implementation a few-shot template in (`get_few_shot_prompt_template`).\n",
    " 2. **Exercise 2.1.2:**  (3 points) Implementation of Few-shot dataloader with independently randomly drawn context.\n",
    " 3. **Exercise 2.1.3:**  (BONUS 10 points) few-shot dataloader with independently drawn semantic context.\n",
    " 4. **Exercise 2.2:**:   (8 points) perform Design of Experiments.\n",
    " 5. **Exercise 2.3:**    (7 points) Analyise and write-up the DoE results.\n",
    "\n",
    "**N.B.** we recommend using Jinja to create templates for prompts. This allows to quickly transform the IMDB samples `text` `str`ings to Few-Shot samples, to be used in your DoE. Additionally, make sure to use `textwrap.dedent` to wrap around triple-quoted (multi-line) `str`ings! Otherwise, you will add (unintentional) whitespace `char`s!\n",
    "\n",
    "> If you find performing 2 and 3 difficult, you can also hard-code some review, and choose an additional system or hyper-parameter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e65ca5cc5be5d6",
   "metadata": {},
   "source": [
    "## 2.1.2 Creating a Few-Shot Template (2 points)\n",
    "First design a template that allows to render a varying number of few shot examples.\n",
    "\n",
    "Your template should take as arguments\n",
    "\n",
    "* `question_answer_pairs` of type `List[Tuple[str, str]]`, i.e., a list of tuples containing a review and a stringified sentiment.\n",
    "* `review` of type `str` that contains the review the model should classify.\n",
    "\n",
    "Your template should render the text in a way that provides the model with examples (`question_answer_pairs`), and then provides the `review` to be classified by the model. You can use your insights from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d68485e1d025ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:26:55.168511Z",
     "start_time": "2024-10-17T19:26:55.159686Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def get_few_shot_prompt_template() -> jinja2.Template:\n",
    "    \"\"\"Function to get a few-shot template to render render Few-Shot prompts in a `dataset.map` function.\n",
    "    \n",
    "    Notes:\n",
    "        The prompt-template uses a variable `question_answer_pair` and `review` as input.\n",
    "        \n",
    "    Examples:\n",
    "        ```\n",
    "        template = get_few_shot_prompt_template()\n",
    "        template.render(question_answer_pair=[('Wow I like this movie', 'Positive'), ('I like the Sequels better...', 'Negative')], review='I like this movie')\n",
    "        ```\n",
    "\n",
    "    Returns:\n",
    "        jinja2.Template that can be rendcered \n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement a few-shot style evaluation prompt, the prompt should use a \n",
    "    #  variable `question_answer_pair`, consisting of a list of Tuples of Reviews and (textual) Labels.\n",
    "    PROMPT_TEMPLATE = textwrap.dedent(\n",
    "        # YOUR CODE GOES HERE\n",
    "        ...\n",
    "        # END OF YOUR CODE\n",
    "    )\n",
    "    assert set(jinja2schema.infer(PROMPT_TEMPLATE).keys()) == {'question_answer_pairs', 'review'}\n",
    "    template = jinja2.Template(PROMPT_TEMPLATE)\n",
    "    return template\n",
    "\n",
    "simple_few_shot_template = get_few_shot_prompt_template()\n",
    "\n",
    "empty_pairs, empty_review = [('', ''), ('', '')], ''\n",
    "empty_template_result = simple_few_shot_template.render(question_answer_pairs=empty_pairs, review=empty_review)\n",
    "example_pairs, example_review = ([('I like the movie', 'Positive'), ('I dislike the movie', 'Negative')], \n",
    "                                 'Event the prequels were far better than this!')\n",
    "example_template_result = simple_few_shot_template.render(question_answer_pairs=example_pairs,review=example_review)\n",
    "\n",
    "display(\n",
    "    Markdown('**Your few-shot prompt looks like this.**'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            {nl_to_br(empty_template_result)}\n",
    "            \"\"\"\n",
    "        )\n",
    "    ),\n",
    "    Markdown('**As an example, your few-shot prompt looks like this.**'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            {nl_to_br(example_template_result)}\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5766737b2a26c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:26:57.911755Z",
     "start_time": "2024-10-17T19:26:57.905168Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Do not edit this cell\n",
    "display(\n",
    "    Markdown('### Exercise 2.1.1 output'),\n",
    "    Markdown('**Few-Shot prompt looks like this.**'),\n",
    "    HTML(\n",
    "    textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "                <tr>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">My few-shot prompt (empty)</th>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">My few-shot prompt (example)</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{nl_to_br(empty_template_result)}</td>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{nl_to_br(example_template_result)}</td>\n",
    "                </tr>\n",
    "            </table>\"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20d8a00678ba1b",
   "metadata": {},
   "source": [
    "## 2.1.2 Create a Few-Shot Dataset (3 points)\n",
    "\n",
    "Next you will complete the implementation to create a Few-Shot `Dataset` that contains the pre-processed few-shot examples, rendered with your template from `2.1.2`. Herein, we will use a `shots` parameter that dictates the size of the context that is provided to the model. Make sure that the shots are randomly drawn for each exercise, but if you find this difficult, hard-coding a set of positive and negative examples is OK as well for 1 out of 3 points.\n",
    "\n",
    "Within this exercise, points are awarded for: \n",
    "\n",
    "* Creating a Dataset with a configurable number of shots (1 point)\n",
    "* Configurable number of randomly drawn shots (2 point)\n",
    "\n",
    "\n",
    "> Note, here you can already set one of the level, by making the `K` of shots configurable, you can also think about the ratio of Positive / Negative.\n",
    "\n",
    "> If you want, and your resources allow for it, you might want to combine the Few-Shot idea with your prompt-based approach, you can use that as a variable, and choose one additional hyper- and/or system-parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe929e482037a3b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:27:15.295719Z",
     "start_time": "2024-10-17T19:27:14.543531Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def draw_batched_random_shots(\n",
    "    batch: Dict[str, List[Any]],\n",
    "    positive_dataset: datasets.Dataset = None,\n",
    "    negative_dataset: datasets.Dataset = None,\n",
    "    template: transformers.PreTrainedTokenizer = None,\n",
    "    shuffle=False,\n",
    "    shots=4,\n",
    ") -> Dict[str, List[Any]]:\n",
    "    \"\"\"Method to implement drawing random shots of data.\n",
    "    Args:\n",
    "        batch (Dict[str, List[str]]): Batch of data to convert to in-context example dataset. \n",
    "        dataset (datasets.Dataset): Dataset to use for drawing random shots. \n",
    "        tokenizer (transformers.PreTrainedTokenizer): the tokenizer to use to convert shot to... \n",
    "        shots (int, 4): Number of shots to sample, defaults to 4. \n",
    "\n",
    "    Returns:\n",
    "        Transformed representation of a batch of samples with the `text` representation updated.\n",
    "    \"\"\"\n",
    "    batch_texts = batch['text']\n",
    "    \"\"\"\n",
    "    Recall that  \n",
    "    text_labels = 'Positive' if label == 0 else 'Negative'\n",
    "    \"\"\"\n",
    "   \n",
    "    # These Lists you need to construct.\n",
    "    result: List[str] = []\n",
    "    positive_shots: List[List[str]] = []\n",
    "    negative_shots: List[List[str]] = []\n",
    "    \n",
    "    # TODO: Implement code to create random contexts of positive and/or negative reviews.\n",
    "    # Hint: use the positive_dataset and negative_dataset\n",
    "    # Hint: dataset can be shuffled, and `take`n from.\n",
    "    # Hint: if you find this difficult, or as additional level you can also hard-code these lists\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    # Merge your sampled or hard-coded shots into a rendered string.\n",
    "    for random_positives, random_negatives, review in zip(positive_shots, negative_shots, batch_texts):\n",
    "        context = random_positives + random_negatives\n",
    "        if shuffle:\n",
    "            random.shuffle(context)\n",
    "        random.shuffle(context)\n",
    "        result.append(\n",
    "            template.render(\n",
    "                question_answer_pairs=context,\n",
    "                review=review\n",
    "            )\n",
    "        )\n",
    "    batch['text'] = result\n",
    "    return batch\n",
    "\n",
    "def get_simple_few_shot_dataset(\n",
    "        train_set: datasets.Dataset,\n",
    "        test_set: datasets.Dataset,\n",
    "        *sample_args,\n",
    "        **sample_kwargs \n",
    ") -> datasets.Dataset:\n",
    "    \"\"\"Function to get a few-shot dataloader that loads random examples from the correst split.\n",
    "    \n",
    "    Args:\n",
    "        train_set (): \n",
    "        test_set (): \n",
    "        shots (int, 4): Number of shots to draw, defaults to 4. \n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    positive_set = train_set.filter(\n",
    "        lambda sample: sample['label'] == 1, batched=False\n",
    "    )\n",
    "    negative_set = train_set.filter(\n",
    "        lambda sample: sample['label'] == 0, batched=False\n",
    "    )\n",
    "    partial_draw_random_shots = partial(draw_batched_random_shots, positive_dataset=positive_set, negative_dataset=negative_set, **sample_kwargs)\n",
    "    return_set = (\n",
    "        test_set\n",
    "        .map(partial_draw_random_shots, batched=True, num_proc=1) # Map to stringified representation\n",
    "    )\n",
    "    return return_set\n",
    "\n",
    "\n",
    "truncated_train_set = (\n",
    "    train_set\n",
    "    .map(truncate_to_50_tokens, batched=True)\n",
    ")\n",
    "truncated_test_set = (\n",
    "    test_set\n",
    "    .map(truncate_to_50_tokens, batched=True)\n",
    ")\n",
    "\n",
    "simple_dataset = get_simple_few_shot_dataset(\n",
    "    truncated_train_set,\n",
    "    truncated_test_set,\n",
    "    template = get_few_shot_prompt_template(),\n",
    "    shots=4\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\"**As an example, here is how your data looks like**\"),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            {nl_to_br(simple_dataset[0]['text'])}\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15d290da26bb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:27:35.473925Z",
     "start_time": "2024-10-17T19:27:35.447404Z"
    },
    "tags": [
     "hide-cell",
     "keep-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Do not edit this code\n",
    "import random\n",
    "index_1, index_2 = random.randint(0, 2500), random.randint(0, 2500)\n",
    "sample_1 = nl_to_br(simple_dataset.shuffle()[index_1]['text'])\n",
    "sample_2 = nl_to_br(simple_dataset.shuffle()[index_2]['text'])\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "                <tr>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">Sample ({index_1})</th>\n",
    "                    <th style=\"text-align: left; border: 1px solid black;\">Sample ({index_2})</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{sample_1}</td>\n",
    "                    <td style=\"text-align: left; border: 1px solid black;\">{sample_2}</td>\n",
    "                </tr>\n",
    "            </table>\"\"\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08c46ea43477f0",
   "metadata": {},
   "source": [
    "### (BONUS) Exercise 2.1.3 I would like additional context please (BONUS 10)\n",
    "\n",
    "Instead of randomly sampling datapoints to create a Few-Shot context. However, maybe we can do better. An example of this, is to create a more semanticly relevant content, that provide more relevant information for the model to make a decision.\n",
    "\n",
    "For this bonus exericse the recipe is (roughly) as follows:\n",
    "\n",
    " 1. Creating a semantic embeddings of samples to create a context from (an embedding model).\n",
    " 2. Creating a vector database to lookup examples.\n",
    " 3. (Pre-compute) set of example to use (I.e. vector lookup).\n",
    " 4. Render the template (similar as before)\n",
    "\n",
    "We have provided some skeleton code to get started, but TAs cannot provide any assistant for this exericse (unless our template contains an error :))\n",
    "\n",
    "> N.B. that this will take some compute power, so you might want to save the (stringified) dataset that you allow to continue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac6a80fd2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "# We advice to use a small model from Sentence transformer, but feel free to use somethign\n",
    "# completey different, or use this as an additonal level!\n",
    "embedding_model = SentenceTransformer(embedding_model)\n",
    "\n",
    "# TODO: Complete and upate the functions to perfrom semantic search.\n",
    "# As a hint: Look at the imports adn see how they can be used.\n",
    "\n",
    "def create_semantic_db(\n",
    "        embedding_model: SentenceTransformer,\n",
    "        train_set: datasets.Dataset,\n",
    "        test_set: datasets.Dataset\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:  \n",
    "    \"\"\"Function to create a sematnic database.\n",
    "    \n",
    "    Args:\n",
    "        embedding_model (): \n",
    "        train_set (): \n",
    "        test_set (): \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Load embeddings if they already exist.\n",
    "    data = None\n",
    "    if (embedding_path := Path('embeddings.pkl')).exists():\n",
    "        with open(embedding_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    \n",
    "    if data is None:\n",
    "        # YOUR CODE GOES HERE\n",
    "        ...\n",
    "        # END OF YOUR CODE\n",
    "    \n",
    "    with open('embeddings.pkl', \"wb\") as fOut:\n",
    "        data = {\n",
    "            'sentences': sentences,\n",
    "            'embeddings': embeddings\n",
    "        }\n",
    "        pickle.dump(data, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    positive_embeddings, negative_embedding = embeddings[:len(embeddings)//2], embeddings[len(embeddings)//2:]\n",
    "    return embeddings, positive_embeddings, negative_embedding\n",
    "\n",
    "def find_batched_semantic_search(\n",
    "        batch: Dict[str, List[str]],\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        corpus_embeddings: torch.Tensor,\n",
    "        negative_corpus_embeddings: torch.Tensor,\n",
    "        positive_corpus_embeddings: torch.Tensor,\n",
    "        shots=4\n",
    ") -> Dict[str, List[str]]:\n",
    "    \n",
    "    # You will have to create a list of rendered string with the found context.\n",
    "    results: List[str] = []\n",
    "    # TODO: Implement a batched `semantic_search` to find relevant items.\n",
    "    \n",
    "    # 1. Perform semantic_search for each item in the batch\n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    "    # 2. Select relevant samples for each sample in the batch\n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    "    # 3. Collate the found results for the batch\n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    "    # 4. Render the results using a template.\n",
    "    # YOUR CODE GOES HERE\n",
    "    ...\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    batch['text'] = results\n",
    "    return batch\n",
    "\n",
    "def get_contextual_drawn_few_shot_dataset(\n",
    "        train_set: datasets.Dataset,\n",
    "        test_set: datasets.Dataset,\n",
    "        shots: int = 4,\n",
    "        *args,\n",
    "        **kwargs\n",
    ") -> datasets.Dataset:\n",
    "    \"\"\"Function to get a few-shot dataloader based on context.\"\"\"\n",
    "    partial_semantic_search = partial(find_batched_semantic_search, train_set=train_set, test_set=test_set, shots=shots)\n",
    "    return_set = (\n",
    "        test_set\n",
    "        .map(partial_semantic_search, batched=True, num_proc=1) # Map to stringified representation\n",
    "    )\n",
    "    return_set.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    return return_set\n",
    "\n",
    "# Step 1: Get embeddings\n",
    "corpus_embedding, positive_embedding, negative_embedding = create_semantic_db(\n",
    "    embedding_model=embedding_model,\n",
    "    train_set=train_set,\n",
    "    test_set=test_set\n",
    ")\n",
    "\n",
    "SHOTS = 4   # YOU MIGHT WANT TO CHANGE THIS IF YOU USE SHOTS AS A VARIABLE\n",
    "q2_complex_set = get_contextual_drawn_few_shot_dataset(\n",
    "    train_set=train_set,\n",
    "    test_set=test_set,\n",
    "    shots=SHOTS\n",
    "    \n",
    ")\n",
    "display(\n",
    "    # TODO: Implement showing an example that shows that it works\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc932d4307f86a75",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Perform DoE (8 points)\n",
    "\n",
    "In this and the following exercise, we are interested in quantifing the effect of different configurations on the Few-Shot performance of the model, you will need to select at-least 3 system- and/or hyper-parameters, with each having at least two or more (2+) levels. Recall that you may wish to use the shots hyper-parameters (e.g. $\\texttt{shots} \\in  [2, 4, 6 ]$).\n",
    "\n",
    "Furthermore, we suggest using one or more from the following parameters in your DoE:\n",
    "\n",
    "  * Model size, for example (`T5-flan-small`, `T5-flan-base`, `T5-flan-large`, etc.). (only recommended with GPU)\n",
    "  * Numerical precision (`torch.float16`, `torch.float32`, `torch.bfloat16`). Make sure your hardware / `PyTorch` version supports this!\n",
    "  * Quantization (only recommended with GPU with `BitsAndBytes` packages).\n",
    "  * Structured decoding (requires implememtantation).\n",
    "\n",
    "\n",
    "In short, you will need to perform;\n",
    "\n",
    "1. (8 points) Design of Experiments in code;\n",
    "    * Selection of critaria.\n",
    "    * Type of factorial experiment.\n",
    "    * Creation of experimental configuration.\n",
    "    * Run your experiments.\n",
    "        * Depending on your chosen variables in DoE, you might need to make some minor adaptations to our provided code.\n",
    "\n",
    "> For your convenience, we have split first DoE part,  and the Design of Experiments (which you have to implement), and the ANOVA analysis into 2 cells. We strongly recommend writing data to disk/persistent storage and loading it in the next cell to make sure you can easily re-run evaluation upon restarting the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ed65e83382979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:35:50.685765Z",
     "start_time": "2024-10-17T19:35:50.678442Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def run_q2_evaluation(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        generation_config,\n",
    "        *args,\n",
    "        **kwargs\n",
    ") -> Tuple[List[List[str]], List[List[int]]]:\n",
    "    \"\"\"Helper function to run evaluation (e.g. under different evaluations.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: \n",
    "        model: \n",
    "        generation_config:\n",
    "        *args: Any additional positional args you want to add.\n",
    "    \n",
    "    Keyword Args:\n",
    "        **kwargs: Any additional keyword args you want to add.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    prediction_list, label_list = [], []\n",
    "    for idx, batch in (pbar := tqdm(enumerate(dataloader), leave=False, total=len(dataloader))):\n",
    "        pbar.set_description(f'Batch {idx}')\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "        \n",
    "        # TODO: You might need to implement something for your experiment here!\n",
    "\n",
    "        # YOUR CODE GOES HERE\n",
    "        ...\n",
    "        # END OF YOUR CODE\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        label_list.append(label.cpu().tolist())\n",
    "    return prediction_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27214eb5a9589b5",
   "metadata": {},
   "source": [
    "### Exericse 2.2.1 Design of Experiments\n",
    "Define your Design of Experiment configurations in the list `EXPERIMENT_CONFIGURATIONS`, you can use this list to store experiment configurations for the different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83544b67b31afedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:39:22.346513Z",
     "start_time": "2024-10-17T19:39:22.342743Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Implement your experimental design here! Decide on hyper-parameters, levels, see Exercise 3 for how to set-this up\n",
    "#  and type of factorial experiment you want to do.\n",
    "\n",
    "EXPERIMENT_CONFIGURATIONS: List[Dict[Any, Any]] = [\n",
    "    None\n",
    "]\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7bf53133827ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:39:22.783140Z",
     "start_time": "2024-10-17T19:39:22.781415Z"
    },
    "tags": [
     "remove-output",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Perform inference. See the cells of Exercise 1 and 3 as a starting point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469b309d8d406d7",
   "metadata": {},
   "source": [
    "### Exercise 1.3 Report on DoE (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e3849cecde170",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Perform your calculations for DoE HERE.\n",
    "\n",
    "# 1. Load data\n",
    "\n",
    "# 2. Create model and fit\n",
    "\n",
    "# 3. Check assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68ccacec068493",
   "metadata": {},
   "source": [
    "\n",
    "> TODO: Write your report here, using appropriate tables, and or $math$, to support your claim.\n",
    "\n",
    "Make sure to clearly state (among others):\n",
    "\n",
    " 1. Which hyper-parameters you are testing.\n",
    " 2. Which levels you are testing for each experiment.\n",
    " 3. How many repetitions you use.\n",
    " 4. Which design of experiment you use: full-factorial / fractional-factorial.\n",
    " 5. Whether the assumptions of the model hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed6320a0a4a80b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfbc09a1c8c754",
   "metadata": {},
   "source": [
    "# Exercise 3: Fine-Tuning Based Classification (20 points total)\n",
    "\n",
    "Lastly, we will perform a fine-tuning based approach, where we will update the model weights in order to 'learn' reply with the clasification of the sentiment of the sentence.\n",
    "\n",
    "**N.B.** We provide most of the code here, as there are multiple non-trivial implementation details. However, the run-time is likely quitea bit longer, so make sure tostart in time.\n",
    "\n",
    "\n",
    "Here we would like to advise to;\n",
    "\n",
    "1. Carefully choose **which hyper-parameters** you want to evaluate, before diving into the implementation, make sure to check that you can reasonably run these experiment within reasonable time.\n",
    "   1. We strongly recommend using a LORA based approach, and focus on; different `target_modules`, `rank`, `alpha`, `drop_out`, and `epochs`.\n",
    "   2. Prefer low values for levels over higher, e.g., a level for epochs can be `1`, or for `steps=100`.\n",
    "   3. You can also try to fine-tune the model, and see whether the fine-tuned model is still capable to perform.\n",
    "   4. If your hardware / pytorch version allows, we also strongly recommend using `bitsandbytes` to further quantize the model, which will speed-up your experiments considerably.\n",
    "2. Preferably run with replication, i.e., at-least a `REPLICATION` of `2`, but if time does not permit for this, a single run is OK as well.\n",
    "3. Look into check-pointing, and recovery, and how much disk-space you need for your experiments.\n",
    "4. Check that you save models to recoverable paths, i.e., you don't overwrite models you train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f89168d58edd3",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Perform DoE (10 points)\n",
    "\n",
    "First, you will need to complete the following code to Design your experiments.\n",
    "\n",
    "> Note, running training will take some time, so make sure to get started early!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feab3d69b11d0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:44:16.033631Z",
     "start_time": "2024-10-17T19:44:15.880504Z"
    },
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "def tokenize_function(\n",
    "        batch,\n",
    "        prefix='Is the following Positive or Negative?\\n',\n",
    "        post_fix='\\nAnswer: '):\n",
    "\n",
    "    updated_text = [f\"{prefix}{review}{post_fix}\" for review in batch[\"text\"]]\n",
    "    batch['text'] = updated_text\n",
    "    # We also set the 'response', i.e., what the model should learn\n",
    "    batch['labels'] = tokenizer(['Positive' if label == 1 else 'Negative' for label in batch[\"label\"]], truncation=True, padding='max_length', return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def train_model(\n",
    "        peft_model,\n",
    "        output_dir: str,\n",
    "        peft_training_args,\n",
    "        train_set,\n",
    "        test_set = None,\n",
    "        \n",
    ") -> Tuple[transformers.Trainer, peft.PeftModel]:\n",
    "    assert output_dir is not None, \"Provide an output dir to save the model\"\n",
    "    assert not Path(output_dir).exists(), \"Provided output dir is not unique!\"\n",
    "    \n",
    "    peft_trainer = transformers.Trainer(\n",
    "        model=original_model,\n",
    "        args=peft_training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=test_set,\n",
    "    )\n",
    "    # Pre-train the model\n",
    "    peft_trainer.train()\n",
    "    # Set the fine-tuned model to evaluate, to remove non-deterministic\n",
    "    #  behavior.\n",
    "    peft_model.eval()\n",
    "    return peft_model, peft_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74abca374d20ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:44:59.951743Z",
     "start_time": "2024-10-17T19:44:59.941272Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Example of hyper-parameters.\n",
    "RANK = 32               # Rank used in model update (lower is faster, less precise)\n",
    "ALPHA = 64              # Scaling factor for update (∆W x dy ALPHA/RANK)           \n",
    "DROPOUT = 0.05          # Regularization term\n",
    "TRAIN_BATCH_SIZE = 32   # Number of samples\n",
    "# GRADIENT_ACCUMULATION_STEPS=1 # If you have low GPU/hardware, you can increase effective batch-size through this.\n",
    "#                               # It 'sums' gradient over GRADIENT_ACCUMULATION_STEPS, to create an effective-batch-size of\n",
    "#                               # GRADIENT_ACCUMULATION_STEPS * TRAIN_BATCH_SIZE\n",
    "TRAIN_EPOCHS = 5        # Total number of trainnig steps.\n",
    "\n",
    "# If you want to save some time, you can store checkpoints, and load them, to create multiple levels\n",
    "# in a single run. Do note, that huggingface by default uses learning-rate scheduling, so this may\n",
    "# affect your results a bit.\n",
    "\n",
    "# The modules are specific to the model itself.\n",
    "MODULES =  ['o'] # Other options are for example, please read the documentation.\n",
    "                 # ['o'], ['k', 'q'], ['q'], ['k', 'q', 'v'], 'or any other identifier of weights.\n",
    "TORCH_DTYPE = torch.float16\n",
    "\n",
    "# TODO: Decide the levels for your experiment. These can be any of the \n",
    "# aforementioned parameters, or any other hyper-parameter.\n",
    "\n",
    "# Hint: Define the levels as a list of numbers for the unique count of \n",
    "#   levels for a parameter.\n",
    "levels: List[int] = ...\n",
    "# Create a list with the names to keep track of the parameters\n",
    "parameters: List[str] = ...\n",
    "# Create a list with levels for each parameter\n",
    "parameter_levels: Dict[str, List[Any]] = ...\n",
    "\n",
    "# EXAMPLE ONLY\n",
    "# Don't actually use this configuration, as this will be a 3 * 2 * 2 * 3 = 36 experiments (without replication)\n",
    "levels = [3, 2, 2, 3]\n",
    "level_names = ['rank', 'alpha', 'dtype',  'epochs']\n",
    "parameter_levels = {\n",
    "    'rank': [8, 16, 32],\n",
    "    'alpha': [16, 32],\n",
    "    'dtype': [torch.float16, torch.float32],\n",
    "    'epochs': [1, 2, 3]\n",
    "}\n",
    "# END OF EXAMPLE \n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d24b5f911ed99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:45:05.382671Z",
     "start_time": "2024-10-17T19:45:05.375124Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Decide the type of (fractional or full) factorial experiment you want to run.\n",
    "# HINT: use the ANOVAandDOE.ipynb notebook as inspiration, and use functinos from pyDOE3\n",
    "import pyDOE3\n",
    "# EXAMPLE ONLY\n",
    "reduction = 4 # for general factorial experiment.\n",
    "experiment = pyDOE3.gsd(\n",
    "    levels, reduction=reduction\n",
    ")\n",
    "# END OF EXAMPLE\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE\n",
    "\n",
    "experiment_configs = pd.DataFrame(\n",
    "    experiment,\n",
    "    columns=[level_names],\n",
    "    \n",
    ")\n",
    "experiment_configs.index.name = 'Experiment ID'\n",
    "\n",
    "\n",
    "display(\n",
    "    experiment_configs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623172be5efbbf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:44:00.615201Z",
     "start_time": "2024-10-17T19:44:00.556395Z"
    },
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "REPETITIONS = 2\n",
    "\n",
    "# If the number of tokens is a level, you might need to change this\n",
    "train_dataset = (\n",
    "    train_set\n",
    "    .map(truncate_to_50_tokens, batched=True)\n",
    "    .map(\n",
    "        tokenize_function, batched=True\n",
    "    )\n",
    "    .map(\n",
    "        lambda batch: fast_tokenizer.batch_encode_plus(\n",
    "            batch['text'],\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "        ), batched=True\n",
    "    )\n",
    ")\n",
    "# Ensure we can effectively use the model\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "EXPERIMENT_CONFIGURATIONS = []\n",
    "for repetition in range(REPETITIONS):\n",
    "    for experiment_id, config_row in enumerate(experiment_configs.iterrows()):\n",
    "        experiment_config = {k[0]: parameter_levels[k[0]][v] for k, v in config_row[1].to_dict().items()}\n",
    "        \n",
    "        EXPERIMENT_CONFIGURATIONS.append(experiment_config)\n",
    "        print(f\"Running experiment: {experiment_id + 1}, repetition: {repetition + 1}\")\n",
    "        print(f\"Experiment config: {experiment_config}\")\n",
    "        \n",
    "        # BEGIN OF YOUR UPDATE TO THIS CODE\n",
    "        rank = experiment_config['rank']\n",
    "        alpha = experiment_config['alpha']\n",
    "        exp_dtype = experiment_config['dtype']\n",
    "        epochs = experiment_config['epochs']\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=rank,\n",
    "            lora_alpha=alpha,\n",
    "            target_modules=MODULES,\n",
    "            lora_dropout=DROPOUT,\n",
    "            bias='none',\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM # Specific for FLAN-T5 model.\n",
    "            # task_type=TaskType.CAUSAL_LM # Specific for Auto-regressive model\n",
    "            # task_type=TaskType.TOKEN_CLS # Specific for Token based classification\n",
    "        )\n",
    "        \n",
    "        original_model, tokenizer, tokenizer_fast = get_model(\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "            torch_dtype=exp_dtype,\n",
    "        )\n",
    "        \n",
    "        output_dir = f'./exercise-3/exp_{repetition}_{experiment_id}_rank={rank}_alpha={alpha}_dtype={exp_dtype}_epochs={epochs}'\n",
    "\n",
    "        \n",
    "        peft_training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            auto_find_batch_size=False,\n",
    "            per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "            learning_rate=1e-4,\n",
    "            num_train_epochs=epochs,\n",
    "            logging_steps=1000,     # You might need to change this, esp. if you subsample the train set.\n",
    "            # max_steps=10000,        # You can use this instead of epochs, for more fine-grained control.\n",
    "            save_total_limit=2,     # Limit the number of checkpoints to save\n",
    "            save_strategy='steps',\n",
    "            save_steps=1000         # You might need to change this\n",
    "        )\n",
    "        # END OF YOUR UPDATE CODE\n",
    "        \n",
    "        peft_model = get_peft_model(\n",
    "            model=original_model,\n",
    "            peft_config=lora_config,\n",
    "        )\n",
    "        peft_model, peft_trainer = train_model(\n",
    "            peft_model=peft_model,\n",
    "            peft_training_args=peft_training_args,\n",
    "            output_dir=output_dir,\n",
    "            train_set=train_dataset,\n",
    "            test_set=None,\n",
    "        )\n",
    "        peft_model.save_pretrained(output_dir)\n",
    "        \n",
    "        del peft_model, peft_trainer\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print('Finished experiment!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a04bd70f05a7",
   "metadata": {
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Next do the evaluation\n",
    "\n",
    "# ONLY SET THIS TO True IFF YOU NEED TO RE-RUN EXPERIMENTS, AS IT WILL\n",
    "#  OVERWRITE YOUR RESULTS.\n",
    "ALLOW_OVERWRITING_RESULTS = False\n",
    "\"\"\"\n",
    "# If you want to experiment with the side of the prompt, you will need to make\n",
    "#  some changes here.\n",
    "\"\"\"\n",
    "# If the number of tokens is a level, you might need to change this\n",
    "test_dataset = (\n",
    "    test_set\n",
    "    .map(truncate_to_50_tokens, batched=True)\n",
    "    .map(\n",
    "        tokenize_function, batched=True\n",
    "    )\n",
    "    .map(\n",
    "        lambda batch: fast_tokenizer.batch_encode_plus(\n",
    "            batch['text'],\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "        ), batched=True\n",
    "    )\n",
    ")\n",
    "# Ensure we can effectively use the model\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "test_datasloader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=15,  # Feel free to lower / higher this\n",
    "            shuffle=False,  # Shuffling not needed during evaluation\n",
    "            num_workers=2,\n",
    "            prefetch_factor=10,\n",
    "        )\n",
    "\n",
    "EXPERIMENT_CONFIGURATIONS = []\n",
    "for experiment_id, config_row in enumerate(experiment_configs.iterrows()):\n",
    "    experiment_config = {k[0]: parameter_levels[k[0]][v] for k, v in config_row[1].to_dict().items()}\n",
    "    \n",
    "    EXPERIMENT_CONFIGURATIONS.append(experiment_config)\n",
    "    \n",
    "original_model, tokenizer, tokenizer_fast = get_model(\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "            torch_dtype=torch.float16, # You might need to change this.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694daaf5b0ae77fa",
   "metadata": {
    "tags": [
     "hide-cell",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "for experiment_id, experiment_config in (exp_bar := tqdm(enumerate(EXPERIMENT_CONFIGURATIONS), leave=True)):\n",
    "    # EXAMPLE CODE HERE\n",
    "    for repetition in tqdm(range(REPETITIONS), leave=False):\n",
    "    \n",
    "        # BEGIN OF YOUR UPDATE TO THIS CODE\n",
    "        rank = experiment_config['rank']\n",
    "        alpha = experiment_config['alpha']\n",
    "        exp_dtype = experiment_config['dtype']\n",
    "        epochs = experiment_config['epochs']\n",
    "        \n",
    "        # END OF YOUR UPDATE TO THIS CODE\n",
    "        # TODO: make sure that your output-dir here has the same format as during training.\n",
    "        output_dir = f'./exercise-3/exp_{repetition}_{experiment_id}_rank={rank}_alpha={alpha}_dtype={exp_dtype}_epochs={epochs}'\n",
    "        \n",
    "        peft_model = peft.PeftModel.from_pretrained(original_model, output_dir)\n",
    "        begin_time = time.time()\n",
    "\n",
    "        prediction_list, label_list = run_q1_evaluation(\n",
    "            test_dataset,  # This you should probably not change\n",
    "            peft_model,  # You might need to change / load a different model for model-parameter\n",
    "            experiment_config,  # You might need to update some kwargs int the generation config for your exp.\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        # Create a flat version to work with.\n",
    "        prediction_list, labels_list = list(chain(*prediction_list)), list(chain(*label_list))\n",
    "        # TODO: Store your results in a way such that you can load it later!\n",
    "        # YOUR CODE GOES HERE\n",
    "        ...\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        # Map the output if we don't recognize it to \n",
    "        label_lut = defaultdict(lambda: -1, {'positive': 1, 'negative': 0})\n",
    "        \n",
    "        predictions = list(map(lambda x: label_lut[x.split(' ')[0].lower()], prediction_list))\n",
    "        \n",
    "        accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, labels_list))) / len(predictions)\n",
    "        unknown =  sum(map(lambda x: x[0] == -1, zip(predictions, labels_list))) / len(predictions)\n",
    "\n",
    "        print(f\"Accuracy ({configuration}): {accuracy}, Unknown: {unknown}\")\n",
    "\n",
    "        \n",
    "        # Write file to disk\n",
    "        save_path = Path(output_dir) / f'result_replication={repetition}.json'\n",
    "        if not save_path.parent.exists():\n",
    "            # Recursively create directory\n",
    "            save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        if save_path.is_file() and not ALLOW_OVERWRITING_RESULTS:\n",
    "            print(\"YOU ARE TRYING TO OVERWRITE AN EXISTING EXPERIMENT FILE!\")\n",
    "            raise Exception(\"Cannot overwrite existing experiment file without `ALLOW_OVERWRITING_RESULTS` flag set.\")\n",
    "        \n",
    "        with open(save_path, 'w') as f:\n",
    "            # TODO: You might want to save some additional results.\n",
    "            json.dump({\n",
    "                \"experiment_config\": experiment_config,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"unknown\": unknown,\n",
    "                \"begin_time\": begin_time,\n",
    "                \"end_time\": end_time,\n",
    "            }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317714c30c754bdc",
   "metadata": {},
   "source": [
    "## Excercise 3.2 Experimental Analysis (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0790513c814d1",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    " # TODO: Put your code here to perform DoE\n",
    " \n",
    " # 1. Load data\n",
    " \n",
    " # 2. Create model and fit\n",
    " \n",
    " # 3. Check assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b248e19a552f6",
   "metadata": {},
   "source": [
    "# TODO: Write your report here, using appropriate tables, and or $math$, to support your claim.\n",
    "\n",
    "Make sure to clearly state (among others):\n",
    "\n",
    "1. Which hyper-parameters you are testing \n",
    "2. Which levels you are testing for each experiment\n",
    "3. How many repetitions you use\n",
    "4. Which design of experiment you use: full-factorial / fractional-factorial.\n",
    "5. Whether the assumptionsn of hte model hold\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
